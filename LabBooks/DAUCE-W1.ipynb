{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2017-12-04 - Installation, VirtualBox et biblio\n",
    "---\n",
    "La première semaine consistera presque exlusivement à de la biblio. Daucé m'a envoyé un ensemble d'articles par mail qu'il faut je lise.\n",
    "\n",
    "Daucé m'a transmit par usb ses scripts python, dont un ~~pour créer un stimulus visuel contenant un chiffre dont la position est variable~~ comprenant un RNA capable de reconnaitre la position d'un stimulus visuel simple, avec une diminution progressive de la précision avec l'éloignement de la cible du point de fixation (**wave-image tensorflow.ipynb**).\n",
    "\n",
    "**Rencontre avec Daucé ce mercredi (2017-12-06)** pour discuter de l'avancement de la biblio.\n",
    "\n",
    "[Cheatsheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet) pour la syntaxe Markdown\n",
    "\n",
    "Je dois me renseigner sur le transfert de données entre ma machine et ma machine virtuelle, notamment pour sauvegarder mes données en cas de problème avec cette dernière. \n",
    "\n",
    "Je vais probablement pouvoir trouver comment dans le [manuel de VirtualBox](http://download.virtualbox.org/virtualbox/UserManual.pdf).\n",
    "\n",
    "J'ai réussi à initier le [transfert de données](https://www.virtualbox.org/manual/ch04.html), puis à résoudre le problème d'[autorisation d'accès au dossier partagé](https://askubuntu.com/questions/890729/this-location-could-not-be-displayed-you-do-not-have-the-permissions-necessary). \n",
    "\n",
    "~~Nouveau problème : le dossier semble vide.\n",
    "Ce problème pourrait être causé par une erreur \"broken shared file\" apparaissant lors de la sélection du dossier à partager.~~  \n",
    "Problème résolu après avoir changé le chemin d'accès du dossier partagé et avoir [redémarré](https://forums.virtualbox.org/viewtopic.php?f=3&t=64191) la machine virtuelle.\n",
    "\n",
    "## Toward predictive machine learning for active vision\n",
    "\n",
    "Article under review de Daucé (?) rassemblant des connaissances sur le sujet.\n",
    "\n",
    "Plus d'informations sur l'**entropie** mathématiques : https://en.wikipedia.org/wiki/Entropy_(information_theory)\n",
    "--> Average amount of information produced by a stochastic source of data. Correspond au negative algorithm of the probability mass function of each value. Donc l'entropie augmente pour les faibles probabilités d'apparition.\n",
    "\n",
    "Plus d'informations sur [argmin](https://math.stackexchange.com/questions/227626/explanation-on-arg-min) --> Argument of the minimum. Exemple : argmin(f(x)) is the value of x for which f(x) attain it's minimum.\n",
    "\n",
    "Plus d'informations sur la [règle de Bayès](https://en.wikipedia.org/wiki/Bayes%27_theorem) :\n",
    "![equation](http://bit.ly/2AJz7Lk)\n",
    "(Voir [ici](https://stackoverflow.com/questions/12502440/markdown-formula-display-in-github) et [là](http://www.sciweavers.org/free-online-latex-equation-editor) pour l'intégration d'images)\n",
    "\n",
    "\n",
    "Plus d'informations sur le [Markov decision process](https://en.wikipedia.org/wiki/Markov_decision_process) --> Mathematical framework for modeling decision making when outcomes are part random and part under control of a decision maker.\n",
    "Voir figure sur feuille.\n",
    "Satisfait la [propriété de Markov](https://en.wikipedia.org/wiki/Markov_property) : Conditionnal probability distribution of future state of the process depends only upon the present state, not on the sequence that preceded it.\n",
    "\n",
    "Plus d'informations sur [softmax](https://en.wikipedia.org/wiki/Softmax_function) --> normalized exponential function\n",
    "\n",
    "---\n",
    "## Tatler et al., 2010 - Yarbus, eye movements and vision\n",
    "\n",
    "[Source de l'article](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3563050/)  \n",
    "Review de la vie et du travail de A.L. Yarbus sur le système visuel. A publié le livre \"Eye Movements and Vision\" en 1965 (traduit vers l'anglais en 1967), toujours l'un des plus cités dans le domaine.  \n",
    "A développé une technique permettant, via des *suction caps*, d'enregistrer précisemment et de façon stable les mouvements des yeux. La technique permettait de maintenir un stimulus visuel dans la fovea, en accordant ses mouvements à ceux des yeux.  \n",
    "Des études utilisant cette technique ont montré que des stimulis maintenus sur la fovea perdent rapidement (1-3s) certaines de leurs caractéristiques (par exemple la différence entre deux parties; ex. ses détails et sa texture), et ceux-ci ne re-apparaissent pas. \n",
    "\n",
    "Cette technique a permis de montrer que les saccades exploratoires de plusieurs sujets d'une même scène visuelle sont **similaires mais pas totalement identiques**. De même lorsque cette même scène est montrée à un même individu avec plusieurs jours d'intervalle, les **saccades exploratoires sont similaires mais pas identiques**, et cette similarité est plus importante que l'inter-individuelle.  \n",
    "De plus, l'exploration visuelle contient des **périodes temporelles** :  \n",
    "+ Pendant la **période initiale** (<35s), les fixations sont fortement **dirigées vers les visages**. Cette préférence est visible dès le début de l'exploration (nb. *indices d'une acquisition rapide d'informations globales sur la scène, permettant de repérer immédiatement les informations d'intérêt, ici les personnes?*)  \n",
    "+ Pendant la suite de l'exploration, on observe une **répétition de cycles** pendant lesquels l'oeil **explore les mêmes éléments importants** (nb. *le système continue d'explorer les stimulis d'intérêts tant que la scène lui est présentée*)   \n",
    "\n",
    "Ces patterns de saccades oculaires sont fortement modulés par la tache réalisée par le sujet, et donc par ce qui représente un stimulus d'intérêt pour lui.   \n",
    "\n",
    "Lors de l'exploration d'un visage, on enregiste ainsi des **cycles d'observations entre le nez, la bouche et les yeux**, avec une nette préférence pour ces derniers. \n",
    "\n",
    "Le travail de Yarbus a permis à Noton et Stark (1971) de développer leur **Scanpath Theory**, qui décrit que les pattern de mouvements oculaires lors d'une exploration visuelle décrivent la perception (nb. *théorie depuis vivement critiquée*).\n",
    "\n",
    "nb. *Fin de l'introduction historique, début de l'étude*\n",
    "\n",
    "21 sujets réalisent une tache d'exploration visuelle de visage pendant un *eye-tracking*. Avant son apparition, une des septs consignes d'exploration est présentée au hasard (ex. estimer l'âge; se souvenir de la position de certains détails).  \n",
    "\n",
    "Obtiennent une **nette tendance à l'exploration de la bouche et d'un oeil** (celui le moins exploré étant ombré), mais le comportement de cycle d'exploration n'est pas retrouvé chez certains sujets.  \n",
    "La profil d'exploration est fortement influencé par la consigne, et certaines entrainent une réduction notable de l'exploration de la bouche et des yeux (points d'intérêt; ex. *remember clothes*).  \n",
    "\n",
    "Division de la photographie de Yarbus (stimulus visuel) en régions d'intérêt pour quantifier l'importance de l'exploration de chacune.  \n",
    "Obtiennent un **effet principal de la région d'intérêt**, plus une **interaction entre la région d'intérêt et l'instruction** (condition).  \n",
    "Influence donc des *priors* sur l'exploration visuelle puisque les instructions sont données avant que celle-ci ait pu débuter.  \n",
    "\n",
    "Ces effets sont moins visibles lorsque le stimulus visuel passe d'un visage seul à un buste.\n",
    "\n",
    "Citent *Tatler, 2009* et *Land and Tatler, 2009* pour plus de renseignements sur les influences bas et haut niveaux sur les mouvements oculaires.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2017-12-05 - Plus de biblio\n",
    "---\n",
    "\n",
    "## Itti and Koch, 1999 - A saliency-based search mechanism for overt and covert shift of visual attention\n",
    "\n",
    "[Source de l'article](http://ilab.usc.edu/publications/doc/Itti_Koch00vr.pdf)  \n",
    "\n",
    "**Carte de saillance** : Carte en deux dimensions encodant la saillance (ou la visibilité) des objets dans l'environnement visuel. La compétition entre les neurones la composant entraîne la victoire d'une seule localisation qui  devient le nouvelle cible (nb. *carte d'attracteurs*). La cible suivante est visée en inhibant la localisation actuelle.  \n",
    "Preuves électrophysiologiques de la présence de ces cartes dans le **pulvinar**, le **colliculus supérieur** et le **sulcus intraparietal**.  \n",
    "(nb. *une partie de la communauté scientifique dément l'existance tel quel de ces cartes, les décrivant comme le résultat de l'intéraction des cartes de caractéristiques*)\n",
    "\n",
    "nb. **Les performances de notre modèle seront comparées à celles du modèle de carte de saillances.** \n",
    "\n",
    "La quantité astronomique d'informations acquise par la rétine excède les capacités de traitement du système : notion de *bottleneck*. Ce problème a été résolu par les systèmes biologiques via la réduction de la quantité d'informations (nb. *fovea*), des systèmes de filtres (nb. *attention*) et un **traitement sériel et préférentiel**. \n",
    "\n",
    "L'attention visuelle semble être contrôlée par deux systèmes coopérants :\n",
    "+ un mécanisme *bottom-up* rapide qui dirige l'attention vers un stimulus sélectionné via sa **sallience**\n",
    "+ un mécanisme *top-down* lent qui dirige l'attention selon des critères variables, sous la direction d'un contrôle volontaire et cognitif\n",
    "\n",
    "Le traitement pré-attentif des informations visuelles ne considère pas toutes les parties d'un scène visuelle de la même façon, mais leur applique un **poids variable** pour ne répondre fortement qu'à certaines d'entre elles, les autres étant faiblement traitées.  \n",
    "Ces poids sont attribués en fonction du **contexte** (nb. *ce qui est présent dans le champs visuel, même en dehors du CR d'un neurone*) et représenteraient la **saillance**.\n",
    "\n",
    "L'équipe a construit un modèle informatique de carte de saillance pour décrire le traitement visuel pré-attentif et montrer qu'il peut réaliser un traitement efficace d'informations provenant d'un de ensemble de 42 cartes de caractéristiques (codant l'intensité, l'orientation et les couleurs).  \n",
    "Leur algorythme ne code que pour un contrôle attentionnel *bottom-up* et pour la localisation d'un stimulus (*where*), mais a obtenu une performance similaire à des sujets humains dans un certain nombre d'expériences.\n",
    "\n",
    "Plus d'informations sur les [pyramides gaussiennes](https://en.wikipedia.org/wiki/Pyramid_(image_processing)).\n",
    "\n",
    "nb. *Pourquoi cette méthode pour modéliser les opérations centre-périphérie?*\n",
    "\n",
    "Plus d'informations sur les [pyramide de Gabor](https://en.wikipedia.org/wiki/Gabor_filter).\n",
    "\n",
    "nb. *\"inhibition form non-classical surround locations is strongest from neurones which are tuned to the same stimulus properties as the center\" ?*\n",
    "\n",
    "Plus d'informations sur la [différence de gaussiennes](https://en.wikipedia.org/wiki/Difference_of_Gaussians).\n",
    "\n",
    "Voir [ce site](http://www.klab.caltech.edu/codedata/codedata.shtml) for the implementation source code.\n",
    "\n",
    "**Rencontre Daucé/Perrinet ce jeudi 2017/12/07** à la cafet CROUS pour du discuter du projet.\n",
    "\n",
    "nb. *lire cet article : http://www.cell.com/cell/abstract/S0092-8674(17)30538-X sur la reconnaissance facile*\n",
    "\n",
    "---\n",
    "## Judd et al., 2009 - Learning to predict where humans look\n",
    "\n",
    "[Source de l'article](http://people.csail.mit.edu/tjudd/WherePeopleLook/Docs/wherepeoplelook.pdf)\n",
    "\n",
    "Pour améliorer l'efficacité du modèle de saillances dans l'exploration visuelle (ie. se baser sur plus que du *bottom-up*), l'équipe a collecté des données d'*eye-tracking* et les a utilisé comme base d'apprentissage et de test pour ce modèle.  \n",
    "Leur base de donnée d'*eye-tracking* et le code MatLab d'acquisition sont disponibles sur le net.  \n",
    "Leur modèle est bien plus performant que ce qui existait alors dans le domaine (ie. il est plus proche de la performance humaine).\n",
    "\n",
    "Citent d'autres modèles plus performants que celui de Itti :\n",
    "+ Bruce et Tsotsos, 2009 : Attention based on Information Maximization (AIM)\n",
    "+ Avhaham and Lindenbaum, 2009 : Esialency\n",
    "+ Cerf et al., 2007\n",
    "Ces modèles sont principalement basés sur des transformations mathématiques et restent moins perfomants que leur modèle, basé sur un apprentissage.\n",
    "\n",
    "Plus d'informations sur la [classification binaire](https://en.wikipedia.org/wiki/Binary_classification).  \n",
    "nb. *\"the saliency map form the fixation locations of one user is treated as a binary classifier on every pixel in the image\" ?*\n",
    "\n",
    "Plus d'informations sur le [*liblinear support vector machine*](https://en.wikipedia.org/wiki/Support_vector_machine) utilisé.  \n",
    "nb. *\"misclassification cost\" ?*\n",
    "\n",
    "nb. *\"horizon detector\" fait partie des \"features\" les plus efficaces pour prédire la localisation des stimulus saillants ?*  \n",
    "\n",
    "---\n",
    "## Freeman and Simoncelli, 2011 - Metamers of the ventral stream\n",
    "\n",
    "[Source de l'article](https://pdfs.semanticscholar.org/1ef9/dd95d3f10f22cd1de602810adf5d07aa906f.pdf)  \n",
    "\n",
    "S'intéresse à l'implication du **courant visuel ventral** (ie. voie du *quoi*, comprenant un ensemble d'aires corticales dont le cortex inférotemporal, impliqué dans la réponse aux objets complexes) dans la reconnaissance visuelle de *pattern* complexes.   \n",
    "Construisent un modèle de l'activité de la population de neurones impliqués, dont les champs récepteurs grandissent avec l'éloignement de V1 et avec l'excentricité au sein d'une aire.   \n",
    "Testent ce modèle en lui imposant des métamères visuels.\n",
    "\n",
    "Plus d'informations sur les [métamères visuels](https://fr.wikipedia.org/wiki/Couleur_m%C3%A9tam%C3%A8re) : stimulis visuels dont les spectres physiques sont différents mais qui sont tout de même perçus identiques. \n",
    "\n",
    "Supposent que l'augmentation des champs récepteurs entraîne une perte irrémédiable d'informations, permettant les phénomènes métamériques pour les stimuli n'était différents que vis-à-vis de l'information perdue.  \n",
    "\n",
    "Leurs expériences leur ont permis de découvrir le *scaling constant* responsable des métamères visuels et qu'elle corresponds à la taille des champs récepteurs des neurones de V2.  \n",
    "Leurs résultats leur ont aussi permis de mettre en évidence les phénomène neurophysiologiques sous-jacents au phénomène de *visual crowding*.\n",
    "\n",
    "Plus d'informations sur le phénomène de [*visual crowding*](https://en.wikipedia.org/wiki/Crowding) : augmentation de la difficulté pour reconnaitre un objet situé dans la vision périphérique s'il est entouré de distracteurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2017-12-06 - Plus de biblio\n",
    "---\n",
    "\n",
    "nb. *Je fini de lire cet article rapidement, je dois jeter un oeil aux autres avant cet après-midi/demain matin. Je pourrais y revenir si besoin*\n",
    "\n",
    "---\n",
    "## Kortum and Geisler, 1996 - Implementation of a foveated image coding system for image bandwidth reduction\n",
    "\n",
    "Construisent un modèle permettant de fortement **réduire en temps réel la résolution d'une image** suivant le gradient observé dans le système visuel (en utilisant une ***square symmetric Carterian resolution structure*** (nb. *cf Fig.2*) pour construire un ***Foveated Imaging System***), et donc de fortement diminuer la bande-passante nécessaire à la transmission de l'information (jusqu'à 94.7%) et donc la **quantité de ressources utilisées**.  \n",
    "\n",
    "Ont validé leur modèle en l'imposant sur des stimulis visuels présentés à des sujets humains (couplé à un *eye-tracking* pour que la partie la plus centrale soit toujours présentée sur la fovea), qui n'ont pas ressenti de différence avec une image non-compressée (lorsque leur attention n'était pas portée spécifiquement vers la périphérie).  \n",
    "Quelques problèmes liés au matériel sont toutefois apparut : la précision élevée de l'*eye-tracking* permettait l'enregistrement de micro-saccades même pendant les fixations ce qui entrainait une légère modification des **SuperPixels** et la perception par les sujets de mouvement dans leur vision périphérique (nb. *il faudra faire attention à ce biais si l'on réalise nous-même de l'eye-tracking*).\n",
    "\n",
    "---\n",
    "## Potthast et al., 2016 - Active multi-view object recognition : A unifying view on online feature selection and view planning\n",
    "\n",
    "Construction d'un **modèle de reconnaissance multi-vues d'objets** réalisant un **apprentissage supervisé**, dont l'un des objectifs est la réduction de la quantité d'énergie consommée par la tache,  et qui unifie deux techniques : la ***online feature selection*** pour réduire le coût computationnel et la ***view planning*** pour résourdre les ambiguités et les occlusions.  \n",
    "Leur modèle peut adopter au moins deux stratégies pour optimiser sa performance: **sélectionner les caractéristiques qui sont d'intérêt pour la reconnaissance**, ou **bouger vers un nouveau point de vue pour réduire l'ambiguité sur l'identité** de la cible.  \n",
    "Leur modèle se base sur une construction **Hidden Markov Model** (**HMM**).\n",
    "\n",
    "La seule sélection en direct des caractéristique leur a permis de **réduire le temps de calcul de 2.5-6 fois**, et le couplage avec la sélection de point de vue d'**augmenter la performance de reconnaissance de 8-18%** (comparé à d'autres modèles qui n'utilisent pas ces deux stratégies).\n",
    "\n",
    "Leur modèle a été implémenté avec succès dans un drone quadrocoptère. \n",
    "\n",
    "nb. *nombreuses formules d'intérêt concernant HMM sur mes notes*\n",
    "\n",
    "nb. *leur modèle ne permet pas la la mise à jour séquentielle de HMM et nécessite l'ensemble des données à l'avance pour ce faire*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2017-12-07 : Mise en place du GitHub\n",
    "---\n",
    "Journée pas très efficace, deux rencontres prévues :  \n",
    "\n",
    "Ce matin, rencontre avec Daucé et Perrinet pour discuter du projet :\n",
    "+ Notre modèle devrait être un **POMDP** (Partially Observed Markov Decision Process), un certain nombre d'informations ajoutées à mes notes manuscrites à ce propos.\n",
    "+ Le modèle de Potthast,2016 est intéressant et on va probablement devoir se situer par rapport à lui dans d'éventuels publications/rapports\n",
    "+ Creation de ma To Do list\n",
    "\n",
    "En début d'après-midi, rencontre avec Vidal pour la signature de la convention M2a.\n",
    "\n",
    "Suivre instructions de mon ancien [LabBook](https://github.com/pierrealbiges/INT-internship/blob/master/W1_Progress_and_notes.ipynb) pour initier git et le transfert de fichiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2017-12-08\n",
    "---\n",
    "\n",
    "Je vais mettre de côté Potthast,2016 de côté pour le moment, il faut que je finisse au plus vite la première partie de ma biblio.\n",
    "\n",
    "---\n",
    "## Najemnik and Geisler, 2005 - Optimal eye movement strategies in visual search\n",
    "\n",
    "Modélisent un **observateur bayesien idéal** pour étudier les statégies optimales de mouvements oculaires dans une tache de détection de cible visuelle.  \n",
    "Leur agent couple les **connaissances de son propre système et de l'environnement observé** (statistiques de la scène) pour réaliser les mouvements oculaires qui lui fourniront le gain d'information le plus important à propos de la position de la cible.  \n",
    "Leur étude leur a permis de mettre en évidence l'**importance capitale du traitement de l'information lors des fixations** et au contraire le **faible bénéfice d'un traitement pendant les mouvements oculaires**. Les systèmes biologiques, notamment humains, traitant majoritairement les informations pendant les fixations, la pression évolutive aurait mené à cette optimisation de l'utilisation des ressources dans le traitement de l'information.  \n",
    "\n",
    "Modèlisent de façon simple les points de fixations optimaux (pour identifier la localisation de la cible) en **cumulant les réponses pondérées provenant de chaque localisation probable de la cible** (voir équations 1 et 2). \n",
    "\n",
    "Comparent le comportement et les résultats de leur modèle avec ceux de sujets humains. Ceux-ci sont très efficaces dans une tache de recheche visuelle, s'approchants de la performance du modèle d'observateur idéal.  \n",
    "La composante MAP de leur modèle est primordiale à sa performance.  \n",
    "Leur modèle présente des similarités avec les sujets humains au delà de ses performances : saccades oculaires biaisées vers la droite et le plus souvent de 3°, majoritairemet localisées à 5° autour du centre du *display*.\n",
    "\n",
    "---\n",
    "## Butko and Movellan, 2010 - Infomax control of eye movements\n",
    "\n",
    "Proposent un modèle plus puissant que les *state-of-the art* au moment de la publication en utilisant les **méthodes infomax** pour apprendre à rechercher une cible visuelle à partir d'**expériences subjectives de récupérations d'informations**.  \n",
    "Leur modèle, en plus d'être très performant, s'adapte de lui-même aux capacités de l'agent mais nécessite de comprendre des modèles probabilistiques d'incertitude sur les systèmes visuel et moteur, et sur le monde extérieur pour être intégré. \n",
    "\n",
    "Quelques infos sur les [algorithmes infomax](https://en.wikipedia.org/wiki/Infomax).  \n",
    "\n",
    "Contient des explications basiques sur les POMDP, leur fonctionnement et quelques uns de leurs problèmes. Voir notes manuscrites.\n",
    "\n",
    "Plus d'informations sur le terme [i.i.d.](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables)\n",
    "\n",
    "Leur modèle **MIPOMDP** ressemble fortement à celui que l'on veut construire, il faudra l'étuder en détails pour les comparer.  \n",
    "\n",
    "Utisent la *dataset* **GENKI** pour récupérer des images contenant des visages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# To Do\n",
    "+ Je dois me renseigner sur les *mountain car models* et sur ZcA (*whitening decorrelation method* correspondant à un pré-traitement de l'image avant de la passer dans les *wavelets* et le modèle)\n",
    "+ ~~Je dois faire une demande pour obtenir gratuitement un compte académique à GitHub~~ (demandé réalisée et acceptée en moins de 10 minutes via [ce lien](https://education.github.com/discount_requests/new)) ~~afin de créer un Hub privé, dans lequel j'inviterai Daucé et Perrinet pour qu'on puisse partager/échanger à propos du projet~~\n",
    "+ Je dois me renseigner sur Mendeley pour gérer efficacement ma biblio, et la partager en ligne avec Daucé et Perrinet\n",
    "+ Je dois m'inscrire sur la liste de diffusion privée de l'INS\n",
    "+ Je dois me renseigner sur TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# A lire\n",
    "+ https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3361132/\n",
    "+ http://bethgelab.org/media/publications/Kuemmerer_High_Low_Level_Fixations_ICCV_2017.pdf\n",
    "+ https://pdfs.semanticscholar.org/0182/5573781674bcf85d0f5d2ec456842f75ad3c.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
