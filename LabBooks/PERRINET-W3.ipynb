{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2018-03-26 - Apprentissage pytorch\n",
    "---\n",
    "## Journal Club InVibe Notes  \n",
    "    Functional brain networks for learning predictive statistics, ? et al., 2017  \n",
    "Only apparition probabilities are kept, signs-letter relation is randomized and changed between subjects and sessions.  \n",
    "0th order markov paradigm (no memory but fixed non-equiprobable probabilities) then 1st order markov paradigm (apparition probabilities directly depends on the last sign that appared).  \n",
    "Notable time differencies between train and evaluation steps, not totally explained by f%RI constraints (unloy used during evaluation steps).   \n",
    "Kullback-Lieber methode used to compute differences between data distributions.  \n",
    "High performance variability between subjects.  \n",
    "Performance seems better when using the maximization (vs matching) strategy but results are too weaks to make any conclusion.  \n",
    "Paprer contrains weird (too far?) interpretations considering methods and results.  \n",
    "Actived areas are differents for strategies and for markov paradigm used.  \n",
    "Biais of using students (not explicit, but subjects have a mean age of 21) for a study investigating learning methodes and implicated cerebral areas?  \n",
    "\n",
    "## connexion\n",
    "J'ai un problème avec la connexion à distance et l'autorisation d'écrire sur les notebooks jupyter: très régulièrement (au moins toutes les 10mn) l'accès est bloqué (FORBIDDEN) empéchant d'enregistrer le travail en cours et nécessitant d'entrer à nouveau le MdP défini pour continuer à travailler.  \n",
    "\n",
    "## Problème actuel à régler: \n",
    "\n",
    "    (...)\n",
    "    x1 = F.relu(F.max_pool1d(self.conv2_drop(self.conv2(x0)), 2))\n",
    "    x2 = x1.view(-1, 480)\n",
    "    x3 = F.relu(self.fc1(x2))\n",
    "    (...)\n",
    "    \n",
    "Les formats sont:\n",
    "\n",
    "    x1: [2000,177]\n",
    "    x3: [480,50]\n",
    "    \n",
    "Pour pouvoir réaliser la troisième ligne de l'apprentissage, il faut utiliser x2 pour changer le format du tenseur, sauf que les tailles sont imcompatibles:\n",
    "\n",
    "    x2 = x1.view(-1,480)\n",
    "    > size '[-1 x 480]' is invalid for input with 234000 elements\n",
    "    \n",
    "Changer l'argument valeur 480 vers 500 résoud ce problème.  \n",
    "\n",
    "Après avoir résolu le problème de calcul du coût (voir notes manuscrites), je me retrouve confronté à une valeur de coût qui augmente exponentiellement :\n",
    "\n",
    "    Train Epoch: 1 [0/60000 (0%)]\tLoss: 165.038376\n",
    "    Train Epoch: 1 [10/60000 (0%)]\tLoss: 224673.953125\n",
    "    Train Epoch: 1 [20/60000 (0%)]\tLoss: 4220524311485612032.000000\n",
    "    Train Epoch: 1 [30/60000 (0%)]\tLoss: nan\n",
    "    Train Epoch: 1 [40/60000 (0%)]\tLoss: nan\n",
    "    \n",
    "Mais au moins l'apprentissage se lance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2018-03-27 - Reducing learning time\n",
    "---\n",
    "L'apprentissage se lance donc, mais il semble avoir besoin d'être fortement optimisé.  \n",
    "Pour première étape, j'ai augmenté la taille de l'échantillon d'apprentissage de 10 à 100, mais l'apprentissage semble très lent.  \n",
    "J'ai ajouté quelques verboses affichant le temps de calcul, et la transformation de chaque image vers le vecteur LogPolar est très longue (donc le calcul des 100 image de l'échantillon d'apprentissage semble interminable):\n",
    "\n",
    "    > time to compute one image (s): 49.64\n",
    "    \n",
    "Plus précisemment, ce sont les transformations réalisées à chaque image qui sont longues :\n",
    "\n",
    "    > Time to reshape one image and process the vectorization (s): 48.93\n",
    "    > Time to compute one image (s): 48.94\n",
    "    \n",
    "Encore plus précisemment, c'est la transformation de l'image par la foncton mnist_reshape_128() qui explique ce temps de calcul important:\n",
    "\n",
    "    > Time to reshape one image (s): 49.60\n",
    "    > Time to reshape one image and process the vectorization (s): 49.61\n",
    "    \n",
    "En simplifiant (passant la variable x_reshape de 4D à 2D) la fonction mnist_reshape_128() pour passer de:\n",
    "\n",
    "    def mnist_reshape_128(x, i_off=0, j_off=0):\n",
    "        N_pix = 28\n",
    "        assert x.shape[2:4] == (N_pix,N_pix)\n",
    "        x_translate = np.zeros((data.shape[0], 1, N_pix*(128/N_pix), N_pix*(128/N_pix)))\n",
    "        x_translate[:,:,(N_pix+22+i_off):(2*N_pix+22+i_off), (N_pix+22+j_off):(2*N_pix+22+j_off)] = x\n",
    "        return x_translate\n",
    "        \n",
    "Vers:\n",
    "\n",
    "    def mnist_reshape_128(x, i_off=0, j_off=0):\n",
    "        N_pix = 28\n",
    "        assert x.shape[2:4] == (N_pix,N_pix)\n",
    "        x_translate = np.zeros((N_pix*(128/N_pix), N_pix*(128/N_pix)))\n",
    "        x_translate[(N_pix+22+i_off):(2*N_pix+22+i_off), (N_pix+22+j_off):(2*N_pix+22+j_off)] = x[2,-1]\n",
    "        return x_translate\n",
    "        \n",
    "Il semblerait que j'ai fortement réduit ce temps de calcul:\n",
    "\n",
    "    Time to reshape one image (s): 0.52\n",
    "    Time to reshape one image and process the vectorization (s): 0.53\n",
    "    Time to compute the whole dataset (s): 53.12\n",
    "    \n",
    "Maintenant que l'entraînement se déroule plus rapidement, il faut que je me penche sur deux choses:  \n",
    "La valeur de perte stagne, l'apprentissage n'est donc pas efficace  \n",
    "L'apprentissage plante après 600 entrées, avec l'erreur suivante:    \n",
    "\n",
    "    x_translate[(N_pix+22+i_off):(2*N_pix+22+i_off), (N_pix+22+j_off):(2*N_pix+22+j_off)] = x[2,-1]\n",
    "    > ValueError: cannot copy sequence with size 28 to array axis with dimension 24    \n",
    "    \n",
    "Après avoir relancé plusieurs fois l'apprentissage, ce crash semble apparaître de façon aléatoire, exemple:\n",
    "\n",
    "    Train Epoch: 1 [1800/60000 (3%)]\tLoss: 159.942780, elapsed time: 16.98 mn\n",
    "    (...)\n",
    "    x_translate[(N_pix+22+i_off):(2*N_pix+22+i_off), (N_pix+22+j_off):(2*N_pix+22+j_off)] = x[2,-1]\n",
    "    # x_translate[104:132,52:80] = x\n",
    "    # 104:132 = 28, 52:80 = 28\n",
    "    # i_off = 54\n",
    "    > ValueError: cannot copy sequence with size 28 to array axis with dimension 24 \n",
    "    \n",
    "    x_translate[(N_pix+22+i_off):(2*N_pix+22+i_off), (N_pix+22+j_off):(2*N_pix+22+j_off)] = x[2,-1]\n",
    "    # x_translate[48:76,-5:23] = x\n",
    "    # 48:76 = 28, -5:23 = 28\n",
    "    # j_off = -55\n",
    "    > ValueError: cannot copy sequence with size 28 to array axis with dimension 0\n",
    "    \n",
    "    x_translate[(N_pix+22+i_off):(2*N_pix+22+i_off), (N_pix+22+j_off):(2*N_pix+22+j_off)] = x[2,-1]\n",
    "    # x_translate[108:136,45:73] = x\n",
    "    # 108:136 = 28, 45:73 = 28\n",
    "    # i_off = 58\n",
    "    > ValueError: cannot copy sequence with size 28 to array axis with dimension 20\n",
    "    \n",
    "Dans tous les cas l'une des coordonnées semble >50, est-ce que l'erreur provient de là?  \n",
    "Après avoir introduit la fonction minmax() pour limiter i_off et j_off à [-50,50], l'erreur ne semble plus apparaître.  \n",
    "\n",
    "Concernant l'évolution du coût au cours de l'apprentissage, après changement de la méthode de calcul, sa valeur semble stagner (après un pic très important en début d'epoch) :\n",
    "\n",
    "    loss = F.mse_loss(OUTPUT, coord, size_average=True)\n",
    "    Training model...\n",
    "    Train Epoch: 1 [0/60000 (0%)]\tLoss: 179.025833, elapsed time: 0.94 mn\n",
    "    Train Epoch: 1 [100/60000 (0%)]\tLoss: 5389.903320, elapsed time: 1.88 mn\n",
    "    Train Epoch: 1 [200/60000 (0%)]\tLoss: 2657.710938, elapsed time: 2.81 mn\n",
    "    Train Epoch: 1 [300/60000 (0%)]\tLoss: 206.353592, elapsed time: 3.74 mn\n",
    "    Train Epoch: 1 [400/60000 (1%)]\tLoss: 208.627625, elapsed time: 4.68 mn\n",
    "    Train Epoch: 1 [500/60000 (1%)]\tLoss: 206.825912, elapsed time: 5.61 mn\n",
    "    Train Epoch: 1 [600/60000 (1%)]\tLoss: 183.498032, elapsed time: 6.54 mn\n",
    "    Train Epoch: 1 [700/60000 (1%)]\tLoss: 217.954712, elapsed time: 7.47 mn\n",
    "    \n",
    "Après avoir augmenté très légèrement le paramètre alpha (passant d'une valeur de 0.01 à 0.03), on observe un sur-apprentissage très important:\n",
    "\n",
    "    Training model...\n",
    "    Train Epoch: 1 [0/60000 (0%)]\tLoss: 192.580429, elapsed time: 0.94 mn\n",
    "    Train Epoch: 1 [100/60000 (0%)]\tLoss: 94805.078125, elapsed time: 1.87 mn\n",
    "    Train Epoch: 1 [200/60000 (0%)]\tLoss: 45865764.000000, elapsed time: 2.80 mn\n",
    "    Train Epoch: 1 [300/60000 (0%)]\tLoss: 365114096091136.000000, elapsed time: 3.74 mn\n",
    "    Train Epoch: 1 [400/60000 (1%)]\tLoss: 6157510041585363631207419192803328.000000, elapsed time: 4.68 mn\n",
    "    Train Epoch: 1 [500/60000 (1%)]\tLoss: 2382964004579637025202740658176.000000, elapsed time: 5.62 mn\n",
    "\n",
    "Avec une valeur d'alpha de 0.02, le sur-apprentissage est moins important mais semble toujours présent:\n",
    "\n",
    "    Train Epoch: 1 [0/60000 (0%)]\tLoss: 173.929565, elapsed time: 0.96 mn\n",
    "    Train Epoch: 1 [100/60000 (0%)]\tLoss: 332601.937500, elapsed time: 1.90 mn\n",
    "    Train Epoch: 1 [200/60000 (0%)]\tLoss: 797965.500000, elapsed time: 2.85 mn\n",
    "    Train Epoch: 1 [300/60000 (0%)]\tLoss: 316.674377, elapsed time: 3.80 mn\n",
    "    Train Epoch: 1 [400/60000 (1%)]\tLoss: 464.111053, elapsed time: 4.75 mn\n",
    "    Train Epoch: 1 [500/60000 (1%)]\tLoss: 1239.878296, elapsed time: 5.71 mn\n",
    "    Train Epoch: 1 [600/60000 (1%)]\tLoss: 2562.166016, elapsed time: 6.66 mn\n",
    "    Train Epoch: 1 [700/60000 (1%)]\tLoss: 4086.354736, elapsed time: 7.61 mn\n",
    "    Train Epoch: 1 [800/60000 (1%)]\tLoss: 6221.288086, elapsed time: 8.57 mn\n",
    "    Train Epoch: 1 [900/60000 (2%)]\tLoss: 8012.681152, elapsed time: 9.52 mn\n",
    "    Train Epoch: 1 [1000/60000 (2%)]\tLoss: 9431.127930, elapsed time: 10.47 mn\n",
    "\n",
    "La solution semblerait de modifier complètement l'optimiseur qu'on utilise (actuellement [SGD](http://pytorch.org/docs/master/optim.html?#torch.optim.SGD))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2018-03-28\n",
    "---\n",
    "Même avec un paramètre alpha d'une valeur de 0.01, on observe après un certain nombre d'échantillons un important sur-apprentissage:\n",
    "\n",
    "    (...)\n",
    "    Train Epoch: 1 [700/60000 (1%)]\tLoss: 222.513367, elapsed time: 7.07 mn\n",
    "    Train Epoch: 1 [800/60000 (1%)]\tLoss: 327.580444, elapsed time: 7.94 mn\n",
    "    Train Epoch: 1 [900/60000 (2%)]\tLoss: 397.229797, elapsed time: 8.82 mn\n",
    "    (...)\n",
    "    Train Epoch: 1 [4100/60000 (7%)]\tLoss: 10234600448.000000, elapsed time: 36.76 mn\n",
    "    Train Epoch: 1 [4200/60000 (7%)]\tLoss: 3827566080.000000, elapsed time: 37.64 mn\n",
    "    Train Epoch: 1 [4300/60000 (7%)]\tLoss: 424681184.000000, elapsed time: 38.52 mn\n",
    "    (...)\n",
    "    \n",
    "Après avoir modifié l'optimiseur vers un [Adam](http://pytorch.org/docs/master/optim.html?#torch.optim.Adam), je n'observe plus de sur-apprentissage pour un paramètre alpha à 0.01, mais une stagnation de la perte:\n",
    "\n",
    "    Train Epoch: 1 [0/60000 (0%)]\tLoss: 187.287643, elapsed time: 0.93 mn\n",
    "    Train Epoch: 1 [100/60000 (0%)]\tLoss: 114.464531, elapsed time: 1.82 mn\n",
    "    Train Epoch: 1 [200/60000 (0%)]\tLoss: 77.370071, elapsed time: 2.75 mn\n",
    "    (...)\n",
    "    Train Epoch: 1 [2700/60000 (4%)]\tLoss: 175.603241, elapsed time: 25.89 mn\n",
    "    Train Epoch: 1 [2800/60000 (5%)]\tLoss: 189.255295, elapsed time: 26.81 mn\n",
    "    Train Epoch: 1 [2900/60000 (5%)]\tLoss: 256.063812, elapsed time: 27.74 mn\n",
    "\n",
    "Après des essais d'apprentissage avec des valeurs d'alpha de 0.03, 0.05, 0.08 et 0.1, le coût semble stagner autour d'une valeur de 200. Peut-être qu'il faudrait changer le graph du réseau?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2018-03-29\n",
    "---\n",
    "L'entrainement CPU est encore plus lent que prévu, et seulement deux epochs ont eu lieu pendant la nuit :\n",
    "\n",
    "    Train Epoch: 1 [59800/60000 (100%)]\tLoss: 204.7736, elapsed time: 545.76 mn\n",
    "    # 545 mn = 9h\n",
    "    \n",
    "Pour chercher ce qui prends tant de temps dans l'apprentissage, je rends le script bavard (temps en % de s) :\n",
    "\n",
    "    time to init pytorch argument: 0.0043\n",
    "    time to init the loaders: 0.053\n",
    "    time to init the nn: 2.02\n",
    "    time to init the optimizer: 0.00025\n",
    "    time to init the mnist_reshape128 and the minmax functions: 0.00035\n",
    "    time to init the vectorization function 0.00018\n",
    "    time to init the train function: 0000.24\n",
    "    time to init the eval function: 0000.26\n",
    "    \n",
    "Aucune initialisation dans le script LP_detect.py ne semble prendre de temps.  \n",
    "Entre temps, j'ai réussi à lancer l'apprenssitage en utilisant l'accélération GPU, mais ça ne semble pas avoir d'effet sur le temps de calcul (qui reste donc similaire à celui n'utilisant que le CPU):\n",
    "\n",
    "    Train Epoch: 1 [0/60000 (0%)]\tLoss: 201.2468, elapsed time: 1.03 mn\n",
    "    Train Epoch: 1 [100/60000 (0%)]\tLoss: 1753.0425, elapsed time: 1.99 mn\n",
    "    Train Epoch: 1 [200/60000 (0%)]\tLoss: 166.1081, elapsed time: 2.94 mn\n",
    "    \n",
    "Encore du bavardage:\n",
    "\n",
    "    time to achieve the vectorization function: 2.6996970176696777\n",
    "    time to achieve the minmax function: 7.3909759521484375e-06\n",
    "    time to achieve the minmax function: 2.1457672119140625e-06\n",
    "    time to achieve the mnist_reshape_128 function: 0.5925393104553223    \n",
    "\n",
    "Pour un échantillon de 100 images:\n",
    "    \n",
    "    vectorization: 2.69 # une seule réalisation par epoch\n",
    "    minmax: 9.53e-4\n",
    "    mnist_reshape_128: 59\n",
    "    \n",
    "Ce serait donc bien le temps de calcul de la fonction mnist_reshape128 qui explique la durée d'apprentissage. Pré-traiter la base de donnée devrait donc fortement réduire ce temps.  \n",
    "\n",
    "J'ai trouvé d'où venait le problème. Lors de l'appel de la fonction mnist_reshape_128():\n",
    "\n",
    "    data_reshaped = mnist_reshape_128(data, i_off, j_off)\n",
    "\n",
    "L'argument \"data\" est de forme [batch_size,1,28,28]; donc à chaque appel :\n",
    "\n",
    "    for idx in range(args.batch_size):\n",
    "        (...)\n",
    "        data_reshaped = mnist_reshape_128(data, i_off, j_off)\n",
    "\n",
    "Je calculais batch_size x batch_size images... Ce qui en plus entraînait la production de 100 échantillons identiques...  \n",
    "Le problème est donc résolu en remplancant le bloc par:\n",
    "\n",
    "    for idx in range(args.eval_batch_size):\n",
    "        (...)\n",
    "        data_reshaped = mnist_reshape_128(data[idx,0,:], i_off, j_off)\n",
    "\n",
    "    Train Epoch: 1 [0/60000 (0%)]\tLoss: 220.5616, elapsed time: 0.08 mn\n",
    "    Train Epoch: 1 [100/60000 (0%)]\tLoss: 431.8232, elapsed time: 0.11 mn\n",
    "    Train Epoch: 1 [200/60000 (0%)]\tLoss: 180.9021, elapsed time: 0.14 mn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# To Do\n",
    "+ **Traduire le modèle de TensorFlow vers Pytorch**\n",
    "### mnist-logPolar-encoding\n",
    "+ Créer un classifier pour stopper les saccades lorsque la cible est identifiée\n",
    "    + ~~Améliorer la méthode d'apprentissage du classifieur (performances très faibles)~~\n",
    "+ Créer une ou plusieurs méthodes pour introduire du bruit écologique dans les images (cf Najemnik2005 (chercher la méthode utilisée dans les sources); librairie [SLIP](https://nbviewer.jupyter.org/github/bicv/SLIP/blob/master/SLIP.ipynb) de Laurent)\n",
    "+ Traduire en modèle probabiliste\n",
    "### Rapport M2b\n",
    "+ **Ecrire une ébauche d'introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# A lire\n",
    "+ http://bethgelab.org/media/publications/Kuemmerer_High_Low_Level_Fixations_ICCV_2017.pdf\n",
    "+ https://pdfs.semanticscholar.org/0182/5573781674bcf85d0f5d2ec456842f75ad3c.pdf\n",
    "+ Schmidhuber, 1991 (voir mail Daucé)\n",
    "+ Parr and Friston, 2017 (voir mail Perrinet)\n",
    "+ http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003005#s1\n",
    "+ http://rpg.ifi.uzh.ch/docs/RAL18_Loquercio.pdf\n",
    "+ https://www.nature.com/articles/sdata2016126\n",
    "+ [Liu et al., 2016](http://ieeexplore.ieee.org/document/7762165/?reload=true) : Learning to Predict Eye Fixations via Multiresolution Convolutional Neural Networks\n",
    "+ [Papier utilisant une méthode similaire à la notre + intégration en robotique](https://www.researchgate.net/publication/220934961_Fast_Object_Detection_with_Foveated_Imaging_and_Virtual_Saccades_on_Resource_Limited_Robots)\n",
    "### Magnocellular pathway function  \n",
    "+ [Selective suppression of the magnocellular visual pathway during saccadic eye movements](http://www.nature.com.lama.univ-amu.fr/articles/371511a0), Burr1994\n",
    "+ [On Identifying Magnocellular and Parvocellular Responses on the Basis of Contrast-Response Functions](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3004196/), Skottun2011\n",
    "+ [Review: Steady and pulsed pedestals, the how and why of post-receptoral pathway separation](http://jov.arvojournals.org/article.aspx?articleid=2191890), Pokorny2011\n",
    "+ [An evolving view of duplex vision: separate but interacting cortical pathways for perception and action](http://www.sciencedirect.com/science/article/pii/S0959438804000340?via%3Dihub), Goodale2004\n",
    "+ [Quantitative measurement of saccade amplitude, duration, and velocity](http://n.neurology.org/content/25/11/1065), Baloh1975\n",
    "### Peripherical vision function\n",
    "+ [The Role of Peripheral Vision in Configural Spatial Knowledge Acquisition](https://etd.ohiolink.edu/pg_10?0::NO:10:P10_ACCESSION_NUM:wright1496188017928082), Douglas2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Satellites\n",
    "+ [Science AMA Series: We’re roboticists at MIT’s Computer Science and Artificial Intelligence Laboratory who developed a soft robot fish that can swim in the ocean. Ask us anything!](https://www.reddit.com/r/science/comments/87hthf/science_ama_series_were_roboticists_at_mits/) ([Archive](https://www.thewinnower.com/papers/8768-science-ama-series-we-re-roboticists-at-mit-s-computer-science-and-artificial-intelligence-laboratory-who-developed-a-soft-robot-fish-that-can-swim-in-the-ocean-ask-us-anything))\n",
    "+ [Harvard Biodesign Lab](https://biodesign.seas.harvard.edu/soft-exosuits)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
