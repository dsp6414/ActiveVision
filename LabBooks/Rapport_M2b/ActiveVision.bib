Automatically generated by Mendeley Desktop 1.17.13
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@misc{SLIP,
abstract = {The SLIP library (https://github.com/bicv/SLIP) defines a simple object-oriented class for gray-scale image processing. Use it to create a SLIP object with a dedicated image size (and optionnaly some other useful parameters) - which you can use to apply common image processing routines to your images.},
author = {Perrinet, Laurent},
keywords = {Image processing,Neuroscience,fourier,perception,textures,vision,whitening},
mendeley-tags = {Image processing,Neuroscience,fourier,perception,textures,vision,whitening},
title = {{SLIP : a Simple Library for Image Processing}},
url = {https://github.com/bicv/SLIP}
}
@article{Kummerer2017,
abstract = {Figure 1: Representative examples for fixation prediction. Fixations are colored depending on whether they are better predicted by the high-level deep object features (DeepGaze II) model (blue) or the low-level intensity contrast features (ICF) model (red). This separates the images into areas where fixations are better predicted by high-level and low-level image features respectively. DeepGaze II is very good at predicting the human tendency to look at text and faces (first and second image), while ICF is better at predicting fixations driven by low-level contrast (third image). In particular, DeepGaze II fails if fixations are primarily driven by low-level features, although high-level features like text are present in the image (fourth image). Abstract Understanding where people look in images is an im-portant problem in computer vision. Despite significant re-search, it remains unclear to what extent human fixations can be predicted by low-level (contrast) compared to high-level (presence of objects) image features. Here we ad-dress this problem by introducing two novel models that use different feature spaces but the same readout architec-ture. The first model predicts human fixations based on deep neural network features trained on object recognition. This model sets a new state-of-the art in fixation predic-tion by achieving top performance in area under the curve metrics on the MIT300 hold-out benchmark (AUC = 88{\%}, sAUC = 77{\%}, NSS = 2.34). The second model uses purely low-level (isotropic contrast) features. This model achieves better performance than all models not using features pre-trained on object recognition, making it a strong baseline to assess the utility of high-level features. We then evaluate and visualize which fixations are better explained by low-level compared to high-level image features. Surprisingly we find that a substantial proportion of fixations are bet-ter explained by the simple low-level model than the state-of-the-art model. Comparing different features within the same powerful readout architecture allows us to better un-derstand the relevance of low-versus high-level features in predicting fixation locations, while simultaneously achiev-ing state-of-the-art saliency prediction.},
author = {K{\"{u}}mmerer, Matthias and Wallis, Thomas S A and Gatys, Leon A and Bethge, Matthias},
doi = {10.1109/ICCV.2017.513},
file = {:home/pimt/Documents/Articles/Kummerer,2017.pdf:pdf},
isbn = {978-1-5386-1032-9},
pages = {4799--4808},
title = {{Understanding Low-and High-Level Contributions to Fixation Prediction}},
url = {http://bethgelab.org/media/publications/Kuemmerer{\_}High{\_}Low{\_}Level{\_}Fixations{\_}ICCV{\_}2017.pdf},
year = {2017}
}
@article{Friston2012,
abstract = {If perception corresponds to hypothesis testing (Gregory, 1980); then visual searches might be construed as experiments that generate sensory data. In this work, we explore the idea that saccadic eye movements are optimal experiments, in which data are gathered to test hypotheses or beliefs about how those data are caused. This provides a plausible model of visual search that can be motivated from the basic principles of self-organized behavior: namely, the imperative to minimize the entropy of hidden states of the world and their sensory consequences. This imperative is met if agents sample hidden states of the world efficiently. This efficient sampling of salient information can be derived in a fairly straightforward way, using approximate Bayesian inference and variational free-energy minimization. Simulations of the resulting active inference scheme reproduce sequential eye movements that are reminiscent of empirically observed saccades and provide some counterintuitive insights into the way that sensory evidence is accumulated or assimilated into beliefs about the world.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Friston, Karl and Adams, Rick A. and Perrinet, Laurent and Breakspear, Michael},
doi = {10.3389/fpsyg.2012.00151},
eprint = {NIHMS150003},
file = {:home/pimt/Documents/Articles/Friston2012.pdf:pdf},
isbn = {1664-1078 (Electronic)},
issn = {16641078},
journal = {Frontiers in Psychology},
keywords = {Active inference,Bayesian inference,Exploration,Free energy,Perception,Salience,Surprise,Visual search},
number = {MAY},
pages = {1--20},
pmid = {22654776},
title = {{Perceptions as hypotheses: Saccades as experiments}},
volume = {3},
year = {2012}
}
@article{Freeman2011,
abstract = {The human capacity to recognize complex visual patterns emerges in a sequence of brain areas known as the ventral stream, beginning with primary visual cortex (V1). We developed a population model for mid-ventral processing, in which nonlinear combinations of V1 responses are averaged in receptive fields that grow with eccentricity. To test the model, we generated novel forms of visual metamers, stimuli that differ physically but look the same. We developed a behavioral protocol that uses metameric stimuli to estimate the receptive field sizes in which the model features are represented. Because receptive field sizes change along the ventral stream, our behavioral results can identify the visual area corresponding to the representation. Measurements in human observers implicate visual area V2, providing a new functional account of neurons in this area. The model also explains deficits of peripheral vision known as crowding, and provides a quantitative framework for assessing the capabilities and limitations of everyday vision.},
author = {Freeman, Jeremy and Simoncelli, Eero P.},
doi = {10.1038/nn.2889},
file = {:home/pimt/Documents/Articles/Freeman2011.pdf:pdf},
isbn = {1546-1726 (Electronic)$\backslash$r1097-6256 (Linking)},
issn = {10976256},
journal = {Nature Neuroscience},
number = {9},
pages = {1195--1204},
pmid = {21841776},
title = {{Metamers of the ventral stream}},
volume = {14},
year = {2011}
}
@book{Werner2014,
edition = {MIT Press},
editor = {Werner, John S. and Chalupa, Leo M.},
isbn = {9780262019163},
keywords = {Neuroscience,Vision,Visual cortex,Visual pathways},
mendeley-tags = {Neuroscience,Vision,Visual cortex,Visual pathways},
pages = {1675},
title = {{The new visual neurosciences}},
year = {2014}
}
@article{Itti2000,
abstract = {Most models of visual search, whether involving overt eye
movements or covert shifts of attention, are based on the
concept of a saliency map, that is, an explicit
two-dimensional map that encodes the saliency or
conspicuity of objects in the visual environment.
Competition among neurons in this map gives rise to a
single winning location that corresponds to the next
attended target. Inhibiting this location automatically
allows the system to attend to the next most salient
location. We describe a detailed computer implementation of
such a scheme, focusing on the problem of combining
information across modalities, here orientation, intensity
and color information, in a purely stimulus-driven manner.
The model is applied to common psychophysical stimuli as
well as to a very demanding visual search task. Its
successful performance is used to address the extent to
which the primate visual system carries out visual search
via one or more such saliency maps and how this can be
tested.},
author = {Itti, Laurent and Koch, Christof},
doi = {10.1016/S0042-6989(99)00163-7},
file = {:home/pimt/Documents/Articles/Itti1999.pdf:pdf},
issn = {0042-6989},
journal = {Vision Research},
keywords = {saliency,vision systems,visual attention},
number = {10-12},
pages = {1489--1506},
title = {{A saliency-based search mechanism for overt and covert hifts of visual attention}},
volume = {40},
year = {2000}
}
@article{Kortum1996,
abstract = {We have developed a preliminary version of a foveated imaging system, implemented on a general purpose computer, which greatly reduces the transmission bandwidth of images. The system is based on the fact that the spatial resolution of the human eye is space variant, decreasing with increasing eccentricity from the point of gaze. By taking advantage of this fact, it is possible to create an image that is almost perceptually indistinguishable from a constant resolution image, but requires substantially less information to code it. This is accomplished by degrading the resolution of the image so that it matches the space-variant degradation in the resolution of the human eye. Eye movements are recorded so that the high resolution region of the image can be kept aligned with the high resolution region of the human visual system. This system has demonstrated that significant reductions in bandwidth can be achieved while still maintaining access to high detail at any point in an image. The system has been tested using 256x256 8 bit gray scale images with 20Â° fields-of-view and eye-movement update rates of 30 Hz (display refresh was 60 Hz). Users of the system have reported minimal perceptual artifacts at bandwidth reductions of up to 94.7{\%} (18.8 times reduction)},
author = {Kortum, Philip and Geisler, Wilson S.},
doi = {10.1117/12.238732},
file = {:home/pimt/Documents/Articles/Kortum1996.pdf:pdf},
isbn = {081942031X},
issn = {0277786X},
journal = {SPIE Proceedings},
keywords = {area-of-interest,eye,field-of-view,foveation,gaze contingent,image compression,movements},
pages = {350--360},
title = {{Implementation of a foveated image coding system for image bandwidth reduction}},
volume = {2657},
year = {1996}
}
@article{Goodale2004,
abstract = {In 1992, Goodale and Milner proposed a division of labour in the visual pathways of the primate cerebral cortex between a dorsal stream specialised for the visual control of action and a ventral stream dedicated to the perception of the visual world. In the years since this original proposal, support for the perception-action hypothesis has come from neuroimaging experiments, human neuropsychology, monkey neurophysiology, and human psychophysical experiments. Indeed, some of the strongest support for this hypothesis has come from behavioural experiments showing that visually guided actions are largely refractory to perceptual illusions. Although controversial, the findings from this literature both support the original hypothesis and suggest important modifications. The ongoing challenge for neurobiologists is to map these behavioural findings onto their corresponding neural substrates.},
author = {Goodale, Melvyn A. and Westwood, David A.},
doi = {10.1016/j.conb.2004.03.002},
file = {:home/pimt/Documents/Articles/Goodale2004.pdf:pdf},
isbn = {0959-4388 (Print)$\backslash$r0959-4388 (Linking)},
issn = {09594388},
journal = {Current Opinion in Neurobiology},
keywords = {AIP,Anterior intraparietal sulcus,Area LO,Functional magnetic resonance imaging,LOC,Lateral occipital area,Lateral occipital complex,MRI,Magnetic resonance imaging,RF,Rod-and-frame,ST,Simultaneous tilt,TMS,Transcranial magnetic stimulation,fMRI},
number = {2},
pages = {203--211},
pmid = {15082326},
title = {{An evolving view of duplex vision: Separate but interacting cortical pathways for perception and action}},
volume = {14},
year = {2004}
}
@article{Potthast2016,
abstract = {Many robots are limited in their operating capabilities, both computational and energy-wise. A strong desire exists to keep computation cost and energy consumption to a minimum when executing tasks like object recognition with a mobile robot. Adaptive action selection is a paradigm, offering great flexibility in trading off the cost of acquiring information against making robust and reliable inference under uncertainty. In this paper, we study active multi-view object recognition and describe an information-theoretic framework that combines and unifies two common techniques: online feature selection for reducing computational costs and view planning for resolving ambiguities and occlusions. Our algorithm adaptively chooses between the two strategies of either selecting only the features that are most informative to the recognition, or moving to a new viewpoint that optimally reduces the expected uncertainty on the identity of the object. This two step process allows us to keep overall computation cost minimal but simultaneously increase recognition accuracy. Extensive empirical studies on a large RGB-D dataset, and with two different feature sets, have validated the effectiveness of the proposed framework. Our experiments show that dynamic feature selection alone reduces the computation time at runtime 2.5â6 times and, when combining it with viewpoint selection, we significantly increase the recognition accuracy on average by 8{\%}â18{\%} absolute compared to systems that do not use these two strategies. By establishing a link between active object recognition and change detection, we were further able to use our framework for the follow-up task of actively detecting object change. Furthermore, we have successfully demonstrated the framework's applicability to a low-powered quadcopter platform with limited operating time.},
author = {Potthast, Christian and Breitenmoser, Andreas and Sha, Fei and Sukhatme, Gaurav S.},
doi = {10.1016/j.robot.2016.06.013},
file = {:home/pimt/Documents/Articles/Potthast2016.pdf:pdf},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {Active perception,Feature selection,Information-theory,Multi-view object recognition,Object change detection,Viewpoint selection},
pages = {31--47},
publisher = {Elsevier B.V.},
title = {{Active multi-view object recognition: A unifying view on online feature selection and view planning}},
url = {http://dx.doi.org/10.1016/j.robot.2016.06.013},
volume = {84},
year = {2016}
}
@article{Butko2010,
author = {Butko, Nicholas J and Movellan, Javier R},
file = {:home/pimt/Documents/Articles/Butko2010.pdf:pdf},
journal = {Autonomous Mental Development, IEEE Transactions on},
number = {2},
pages = {91--107},
title = {{Infomax control of eye movements}},
volume = {2},
year = {2010}
}
@article{Denison2014,
abstract = {The magnocellular (M) and parvocellular (P) subdivisions of primate LGN are known to process complementary types of visual stimulus information, but a method for noninvasively defining these subdivisions in humans has proven elusive. As a result, the functional roles of these subdivisions in humans have not been investigated physiologically. To functionally map the M and P subdivisions of human LGN, we used high-resolution fMRI at high field (7. T and 3. T) together with a combination of spatial, temporal, luminance, and chromatic stimulus manipulations. We found that stimulus factors that differentially drive magnocellular and parvocellular neurons in primate LGN also elicit differential BOLD fMRI responses in human LGN and that these responses exhibit a spatial organization consistent with the known anatomical organization of the M and P subdivisions. In test-retest studies, the relative responses of individual voxels to M-type and P-type stimuli were reliable across scanning sessions on separate days and across sessions at different field strengths. The ability to functionally identify magnocellular and parvocellular regions of human LGN with fMRI opens possibilities for investigating the functions of these subdivisions in human visual perception, in patient populations with suspected abnormalities in one of these subdivisions, and in visual cortical processing streams arising from parallel thalamocortical pathways. {\textcopyright} 2014 Elsevier Inc.},
archivePrefix = {arXiv},
arxivId = {15334406},
author = {Denison, Rachel N. and Vu, An T. and Yacoub, Essa and Feinberg, David A. and Silver, Michael A.},
doi = {10.1016/j.neuroimage.2014.07.019},
eprint = {15334406},
file = {:home/pimt/Documents/Articles/Denison2014.pdf:pdf},
isbn = {1095-9572 (Electronic) 1053-8119 (Linking)},
issn = {10959572},
journal = {NeuroImage},
keywords = {7T,FMRI,Lateral geniculate nucleus,Magnocellular,Parallel processing,Parvocellular},
number = {P2},
pages = {358--369},
pmid = {25038435},
title = {{Functional mapping of the magnocellular and parvocellular subdivisions of human LGN}},
volume = {102},
year = {2014}
}
@book{Zhaoping2014,
author = {Zhaoping, Li},
edition = {Oxford Uni},
isbn = {9780199564668},
keywords = {Mathematics,Neuroscience,Vison},
mendeley-tags = {Mathematics,Neuroscience,Vison},
pages = {383},
title = {{Understanding vision : theory, models and data}},
year = {2014}
}
@article{Judd,
abstract = {For many applications in graphics, design, and human computer interaction, it is essential to understand where humans look in a scene. Where eye tracking devices are not a viable option, models of saliency can be used to pre-dict fixation locations. Most saliency approaches are based on bottom-up computation that does not consider top-down image semantics and often does not match actual eye move-ments. To address this problem, we collected eye tracking data of 15 viewers on 1003 images and use this database as training and testing examples to learn a model of saliency based on low, middle and high-level image features. This large database of eye tracking data is publicly available with this paper.},
author = {Judd, Tilke and Ehinger, Krista and Durand, Fr{\'{e}}do and Torralba, Antonio},
file = {:home/pimt/Documents/Articles/Judd2009.pdf:pdf},
isbn = {9781424444199},
title = {{Learning to Predict Where Humans Look}},
url = {http://ieeexplore.ieee.org/ielx5/5453389/5459144/05459462.pdf?tp={\&}arnumber=5459462{\&}isnumber=5459144}
}
@article{Tatler2010,
abstract = {The impact of Yarbus's research on eye movements was enormous following the translation of his book Eye Movements and Vision into English in 1967. In stark contrast, the published material in English concerning his life is scant. We provide a brief biography of Yarbus and assess his impact on contemporary approaches to research on eye movements. While early interest in his work focused on his study of stabilised retinal images, more recently this has been replaced with interest in his work on the cognitive influences on scanning patterns. We extended his experiment on the effect of instructions on viewing a picture using a portrait of Yarbus rather than a painting. The results obtained broadly supported those found by Yarbus.},
author = {Tatler, Benjamin W. and Wade, Nicholas J. and Kwan, Hoi and Findlay, John M. and Velichkovsky, Boris M.},
doi = {10.1068/i0382},
file = {:home/pimt/Documents/Articles/Tatler2010.pdf:pdf},
isbn = {2041-6695},
issn = {20416695},
journal = {i-Perception},
keywords = {Eye guidance,Eye movement,Face perception,History,Saccade,Scene perception,Stabilised retinal image,Yarbus},
number = {1},
pages = {7--27},
pmid = {23396904},
title = {{Yarbus, eye movements, and vision}},
volume = {1},
year = {2010}
}
@article{Fischer2007,
author = {Fischer, Sylvain and Perrinet, Laurent and Redondo, Rafael and Cristobal, Gabriel},
doi = {10.1007/s11263-006-0026-8},
file = {:home/pimt/Documents/Articles/Fischer2007.pdf:pdf},
journal = {International Journal of Computer Vision},
keywords = {ARTIFICIAL INTELLIGENCE,Artificial Intelligence (incl. Robotics),COMPRESSION,COMPUTER SCIENCE,CORTICAL-CELLS,Computer Imaging,Computer Science,FILTERS,IMAGE REPRESENTATION,Image Processing and Computer Vision,NATURAL IMAGES,Pattern Recognition,Pattern Recognition and Graphics,RESPONSES,SPATIAL-FREQUENCY,STATISTICS,Studies,TRANSFORMS,VISUAL-CORTEX,Vision,Wavelet transforms,image denoising,log-Gabor filters,oriented high-pass filters,visual system,wavelet transforms},
number = {2},
pages = {231--246},
title = {{Self-invertible 2D log-Gabor wavelets}},
url = {http://invibe.net/LaurentPerrinet/Publications/Fischer07cv?action=AttachFile{\&}do=view{\&}target=Fischer07cv.pdf},
volume = {75},
year = {2007}
}
@article{Uddin2004,
author = {Uddin, M K and Ninose, Y and Nakamizo, S},
doi = {10.2117/psysoc.2004.28},
file = {:home/pimt/Documents/Articles/Uddin2004.pdf:pdf},
issn = {00332852},
journal = {Psychologia},
keywords = {1993,accuracy of spatial localization,adam,and hoek,course and,in which they demonstrated,ketelaars,kingma,memory-guided saccade,reported on the time,spatial localization,that localization performance,two-process model,visually guided saccade},
pages = {28--34},
title = {{Accuracy and precision of spatial localization with and without saccadic eye movements: A test of the two-process model}},
volume = {47},
year = {2004}
}
@article{Najemnik2005,
author = {Najemnik, J and Geisler, Wilson S.},
file = {:home/pimt/Documents/Articles/Najemnik2005.pdf:pdf},
journal = {Nature reviews. Neuroscience},
pages = {387--391},
title = {{Optimal eye movement strategies in visual search}},
url = {http://dx.doi.org/10.1038/nature03390},
volume = {434},
year = {2005}
}
