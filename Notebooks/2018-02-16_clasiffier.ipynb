{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning a classifier\n",
    "\n",
    "On commence par la fonction de base apprise de la librairie torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T15:21:53.665546Z",
     "start_time": "2018-02-16T15:21:53.652605Z"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile /tmp/main.py\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                    help='input batch size for training (default: 64)')\n",
    "parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "                    help='input batch size for testing (default: 1000)')\n",
    "parser.add_argument('--epochs', type=int, default=10, metavar='N',\n",
    "                    help='number of epochs to train (default: 10)')\n",
    "parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n",
    "                    help='learning rate (default: 0.01)')\n",
    "parser.add_argument('--momentum', type=float, default=0.5, metavar='M',\n",
    "                    help='SGD momentum (default: 0.5)')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='disables CUDA training')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                    help='random seed (default: 1)')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "args = parser.parse_args()\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('/tmp/data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('/tmp/data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args.test_batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "model = Net()\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if args.cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "\n",
    "def test(test_loader=test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        if args.cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T15:21:58.395580Z",
     "start_time": "2018-02-16T15:21:56.274410Z"
    }
   },
   "outputs": [],
   "source": [
    "%run /tmp/main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "On apprend une matrice de poids qui est fixée dans la suite et que nous allons utiliser pour la classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T15:21:54.071929Z",
     "start_time": "2018-02-16T15:21:54.055314Z"
    }
   },
   "outputs": [],
   "source": [
    "path = \"MNIST_classifier.pt\"\n",
    "\n",
    "import os\n",
    "if os.path.isfile(path):\n",
    "    print('Loading')\n",
    "    model.load_state_dict(torch.load(path))\n",
    "else:\n",
    "    print('Learning')\n",
    "    import time\n",
    "    t0 = time.time()\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train(epoch)\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print('Done in ', time.time() - t0, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T15:21:56.272288Z",
     "start_time": "2018-02-16T15:21:54.073766Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the learned classifier in a standalone class\n",
    "\n",
    "Maintenant qu'on a appris les points qui permet une classification d'à peu près 98 % on va utiliser le modèle fead-forward pour faire la classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T15:21:58.395580Z",
     "start_time": "2018-02-16T15:21:56.274410Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ParvoNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ParvoNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "import torch\n",
    "test_batch_size = 1000\n",
    "test_batch_size = 20\n",
    "cmin, cmax = 0.1307, 0.3081\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "datasets.MNIST('/tmp/data', train=False, transform=transforms.Compose([\n",
    "                   transforms.ToTensor(),\n",
    "                   transforms.Normalize((cmin,), (cmax,))\n",
    "               ])), batch_size=test_batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "\n",
    "def test(test_loader=test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    return correct / len(test_loader.dataset)\n",
    "\n",
    "\n",
    "model = ParvoNet()\n",
    "path = \"MNIST_classifier.pt\"\n",
    "model.load_state_dict(torch.load(path))\n",
    "\n",
    "test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## shifting the input images\n",
    "\n",
    "\n",
    "Je vais maintenant générer des données en utilisant les données originales de MNIST translatées :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T15:21:58.402234Z",
     "start_time": "2018-02-16T15:21:58.397909Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from torchvision import datasets, transforms\n",
    "i_shift, j_shift = 12, 17\n",
    "N_pix = 28\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T15:21:58.788582Z",
     "start_time": "2018-02-16T15:21:58.405020Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "for data, target in test_loader:\n",
    "    break\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(data[0, 0, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T15:21:59.016567Z",
     "start_time": "2018-02-16T15:21:58.791167Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data_translate = data.min() * np.ones((data.shape[0], 1, N_pix*3 - 2, N_pix*3 - 2))\n",
    "print(data_translate.shape)\n",
    "data_translate[:, :, (N_pix-i_shift):(2*N_pix-i_shift), (N_pix-j_shift):(2*N_pix-j_shift)] = data\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(data_translate[0, 0, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T15:21:59.207642Z",
     "start_time": "2018-02-16T15:21:59.019469Z"
    }
   },
   "outputs": [],
   "source": [
    "data_cropped = data_translate[:, :, (N_pix):(2*N_pix), (N_pix):(2*N_pix)]\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(data_cropped[0, 0, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T15:21:59.218565Z",
     "start_time": "2018-02-16T15:21:59.213275Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.arange(-N_pix+1, N_pix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T15:21:59.420090Z",
     "start_time": "2018-02-16T15:21:59.221082Z"
    }
   },
   "outputs": [],
   "source": [
    "def shift_data(data, i_shift, j_shift):\n",
    "    N_pix = data.shape[-1]\n",
    "    assert(N_pix == data.shape[-2])\n",
    "    import numpy as np\n",
    "    data_translate = data.min() * np.ones((data.shape[0], 1, N_pix*3 - 1, N_pix*3 - 1))\n",
    "    data_translate[:, :, (N_pix+i_shift):(2*N_pix+i_shift), (N_pix+j_shift):(2*N_pix+j_shift)] = data\n",
    "    data_cropped = data_translate[:, :, (N_pix):(2*N_pix), (N_pix):(2*N_pix)]\n",
    "    return data_cropped\n",
    "\n",
    "data_cropped = shift_data(data, i_shift = 12, j_shift = -12)\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(data_cropped[0, 0, :, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the learned classifier on the shifted data\n",
    "\n",
    "On peut maintenant tester le classifieur sur les images Translatées en calculant la valeur de classification en  fonction de l'erreur de localisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T16:20:03.447637Z",
     "start_time": "2018-02-16T16:17:27.210387Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def test_shift(test_loader, i_shift, j_shift, verbose=0):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data_cropped = shift_data(data, i_shift=i_shift, j_shift=j_shift)        \n",
    "        data_cropped = torch.FloatTensor(data_cropped) #transforms.ToTensor()(data_cropped)\n",
    "        data_cropped, target = Variable(data_cropped, volatile=True), Variable(target)\n",
    "        output = model(data_cropped)\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    if verbose: print('\\nTest set: at ({}, {}), the  average loss is {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        i_shift, j_shift, test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    return correct / len(test_loader.dataset)\n",
    "\n",
    "\n",
    "path = \"MNIST_accuracy.pt\"\n",
    "\n",
    "import os\n",
    "if os.path.isfile(path):\n",
    "    print('Loading')\n",
    "    accuracy = np.load(path)\n",
    "else:\n",
    "    print('Learning')\n",
    "    import time\n",
    "    t0 = time.time()\n",
    "    accuracy = np.zeros((2*N_pix-1, 2*N_pix-1))\n",
    "    from tqdm import tqdm\n",
    "    N_step = 5\n",
    "\n",
    "    with tqdm(total=(2*N_pix-1)**2) as pbar:\n",
    "        for i_shift in np.arange(-N_pix+1, N_pix, N_step):\n",
    "            for j_shift in np.arange(-N_pix+1, N_pix, N_step):\n",
    "                accuracy[i_shift+N_pix-1, j_shift+N_pix-1] = test_shift(test_loader, i_shift, j_shift)\n",
    "                pbar.update()\n",
    "    np.save(accuracy, path)\n",
    "    print('Done in ', time.time() - t0, 'seconds')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "J'avoue que c'est un peu bourrin de calculer la classification sur les 128 × 128 pixels pour 1000 batch multiplié par 10 type d'entrées.... Mais bon on doit faire ça seulement une fois :-)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T16:15:33.975808Z",
     "start_time": "2018-02-16T16:15:33.970851Z"
    }
   },
   "outputs": [],
   "source": [
    "np.arange(-N_pix+1, N_pix, N_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T15:42:24.769583Z",
     "start_time": "2018-02-16T15:21:53.667Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.pcolor(np.arange(-N_pix+1, N_pix)[:, np.newaxis], np.arange(-N_pix+1, N_pix)[np.newaxis, :], accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T15:42:24.771297Z",
     "start_time": "2018-02-16T15:21:53.670Z"
    }
   },
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette fonction de performance du classifieur  est calculée indépendamment de la forme spécifique du chiffre entre 0 et 9. Elle donne donc la carte de performance qu'on attend Au niveau de la classification/ On va pouvoir maintenant l'utiliser ceomm label pour apprendre de façon supervisée la correspondance entre la carte log-polaire obtenue depuis l'image brute et cette carte de performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
