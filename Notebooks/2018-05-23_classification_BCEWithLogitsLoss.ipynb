{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From https://pytorch.org/docs/0.3.1/nn.html?highlight=crossentropy#bcewithlogitsloss\n",
    "\n",
    "This loss combines a Sigmoid layer and the BCELoss in one single class. This version is more numerically stable than using a plain Sigmoid followed by a BCELoss as, by combining the operations into one layer, we take advantage of the log-sum-exp trick for numerical stability.\n",
    "\n",
    "The loss can be described as:\n",
    "ℓ(x,y)=L={l1,…,lN}⊤,ln=−wn[tn⋅logσ(xn)+(1−tn)⋅log(1−σ(xn))],\n",
    "\n",
    "where N\n",
    "\n",
    "is the batch size. If reduce is True, then\n",
    "ℓ(x,y)={mean(L),sum(L),ifsize_average=True,ifsize_average=False.\n",
    "\n",
    "This is used for measuring the error of a reconstruction in for example an auto-encoder. Note that the targets t[i] should be numbers between 0 and 1.\n",
    "Parameters:\t\n",
    "\n",
    "    weight (Tensor, optional) – a manual rescaling weight given to the loss of each batch element. If given, has to be a Tensor of size “nbatch”.\n",
    "    size_average – By default, the losses are averaged over observations for each minibatch. However, if the field size_average is set to False, the losses are instead summed for each minibatch. Default: True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-23T12:48:07.719392Z",
     "start_time": "2018-05-23T12:48:06.321600Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "#import noise\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import MotionClouds as mc\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from LogGabor import LogGabor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charger la matrice de certitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-23T12:48:07.798260Z",
     "start_time": "2018-05-23T12:48:07.759455Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading accuracy...\n",
      "[[0.0974 0.0974 0.0974 ... 0.0974 0.0974 0.0974]\n",
      " [0.0974 0.0974 0.0974 ... 0.0974 0.0974 0.0974]\n",
      " [0.0974 0.0974 0.0974 ... 0.0974 0.0974 0.0974]\n",
      " ...\n",
      " [0.0974 0.0974 0.0974 ... 0.0974 0.0974 0.0974]\n",
      " [0.0974 0.0974 0.0974 ... 0.0974 0.0974 0.0974]\n",
      " [0.0974 0.0974 0.0974 ... 0.0974 0.0974 0.0974]]\n"
     ]
    }
   ],
   "source": [
    "path = \"MNIST_accuracy.npy\"\n",
    "if os.path.isfile(path):\n",
    "    print('Loading accuracy...')\n",
    "    accuracy =  np.load(path)\n",
    "    print(accuracy)\n",
    "else:\n",
    "    print('No accuracy data found.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Préparer l'apprentissage et les fonctions nécessaires au fonctionnement du script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-23T12:48:13.146359Z",
     "start_time": "2018-05-23T12:48:07.807303Z"
    }
   },
   "outputs": [],
   "source": [
    "N_theta, N_orient, N_scale, N_phase, N_X, N_Y, rho = 6, 12, 5, 2, 128, 128, 1.61803\n",
    "\n",
    "def vectorization(N_theta, N_orient, N_scale, N_phase, N_X, N_Y, rho):\n",
    "    phi = np.zeros((N_theta, N_orient, N_scale, N_phase, N_X*N_Y))\n",
    "    parameterfile = 'https://raw.githubusercontent.com/bicv/LogGabor/master/default_param.py'\n",
    "    lg = LogGabor(parameterfile)\n",
    "    lg.set_size((N_X, N_Y))\n",
    "    params = {'sf_0': .1, 'B_sf': 2*lg.pe.B_sf,\n",
    "              'theta': np.pi * 5 / 7., 'B_theta': 2*lg.pe.B_theta}\n",
    "    phase = np.pi/4\n",
    "    edge = lg.normalize(lg.invert(lg.loggabor(\n",
    "        N_X/3, 3*N_Y/4, **params)*np.exp(-1j*phase)))\n",
    "\n",
    "    for i_theta in range(N_theta):\n",
    "        for i_orient in range(N_orient):\n",
    "            for i_scale in range(N_scale):\n",
    "                ecc = (1/rho)**(N_scale - i_scale)\n",
    "                r = np.sqrt(N_X**2+N_Y**2) / 2 * ecc  # radius\n",
    "                sf_0 = 0.5 * 0.03 / ecc\n",
    "                x = N_X/2 + r * \\\n",
    "                    np.cos((i_orient+(i_scale % 2)*.5)*np.pi*2 / N_orient)\n",
    "                y = N_Y/2 + r * \\\n",
    "                    np.sin((i_orient+(i_scale % 2)*.5)*np.pi*2 / N_orient)\n",
    "                for i_phase in range(N_phase):\n",
    "                    params = {'sf_0': sf_0, 'B_sf': lg.pe.B_sf,\n",
    "                              'theta': i_theta*np.pi/N_theta, 'B_theta': np.pi/N_theta/2}\n",
    "                    phase = i_phase * np.pi/2\n",
    "                    phi[i_theta, i_orient, i_scale, i_phase, :] = lg.normalize(\n",
    "                        lg.invert(lg.loggabor(x, y, **params)*np.exp(-1j*phase))).ravel()\n",
    "    return phi\n",
    "\n",
    "\n",
    "phi = vectorization(N_theta, N_orient, N_scale, N_phase, N_X, N_Y, rho)\n",
    "phi_vector = phi.reshape((N_theta*N_orient*N_scale*N_phase, N_X*N_Y))\n",
    "phi_plus = np.linalg.pinv(phi_vector)\n",
    "\n",
    "energy = (phi**2).sum(axis=(0, 3))\n",
    "energy /= energy.sum(axis=-1)[:, :, None]\n",
    "energy_vector = energy.reshape((N_orient*N_scale, N_X*N_Y))\n",
    "energy_plus = np.linalg.pinv(energy_vector)\n",
    "\n",
    "def accuracy_128(i_offset, j_offset, N_pic=128, N_stim=55):\n",
    "    center = (N_pic-N_stim)//2\n",
    "\n",
    "    accuracy_128 = 0.1 * np.ones((N_pic, N_pic))\n",
    "    accuracy_128[(center+i_offset):(center+N_stim+i_offset),\n",
    "                 (center+j_offset):(center+N_stim+j_offset)] = accuracy\n",
    "\n",
    "    accuracy_LP = energy_vector @ np.ravel(accuracy_128)\n",
    "    return accuracy_LP\n",
    "\n",
    "\n",
    "def mnist_128(data, i_offset, j_offset, N_pic=128, N_stim=28, noise=True, noise_type='MotionCloud'):\n",
    "    center = (N_pic-N_stim)//2\n",
    "    #data_128 = np.zeros((N_pic, N_pic))\n",
    "    data_128 = (data.min().numpy()) * np.ones((N_pic, N_pic))\n",
    "\n",
    "    data_128[int(center+i_offset):int(center+N_stim+i_offset),\n",
    "             int(center+j_offset):int(center+N_stim+j_offset)] = data\n",
    "\n",
    "    if noise:\n",
    "        if noise_type == 'MotionCloud':\n",
    "            data_LP = phi_vector @ np.ravel(data_128 + MotionCloudNoise())\n",
    "        elif noise_type == 'Perlin':\n",
    "            data_LP = phi_vector @ np.ravel(\n",
    "                data_128 + randomized_perlin_noise())\n",
    "    else:\n",
    "        data_LP = phi_vector @ np.ravel(data_128)\n",
    "    return data_LP\n",
    "\n",
    "\n",
    "def couples(data, i_offset, j_offset):\n",
    "    v = mnist_128(data, i_offset, j_offset)\n",
    "    a = accuracy_128(i_offset, j_offset)\n",
    "    return (v, a)\n",
    "\n",
    "\n",
    "def minmax(value, border):\n",
    "    value = max(value, -border)\n",
    "    value = min(value, border)\n",
    "    return value\n",
    "\n",
    "\n",
    "def sigmoid(values):\n",
    "    values = 1 / (1 + ((1 / 0.1) - 1) * np.exp(-values))\n",
    "    return values\n",
    "\n",
    "\n",
    "def randomized_perlin_noise(shape=(128, 128), scale=10, octaves=6, persistence=0.5, lacunarity=2.0, base=0):\n",
    "    noise_vector = np.zeros(shape)\n",
    "    for i in range(shape[0]):\n",
    "        for j in range(shape[1]):\n",
    "            noise_vector[i][j] = noise.pnoise2(i/scale,\n",
    "                                               j/scale,\n",
    "                                               octaves=int(\n",
    "                                                   octave * abs(np.random.randn()))+1,\n",
    "                                               persistence=persistence *\n",
    "                                               abs(np.random.randn()),\n",
    "                                               lacunarity=lacunarity *\n",
    "                                               abs(np.random.randn()),\n",
    "                                               repeatx=shape[0],\n",
    "                                               repeaty=shape[1],\n",
    "                                               base=base)\n",
    "    return noise_vector\n",
    "\n",
    "\n",
    "def MotionCloudNoise(sf_0=0.125, B_sf=3.):\n",
    "    mc.N_X, mc.N_Y, mc.N_frame = 128, 128, 1\n",
    "    fx, fy, ft = mc.get_grids(mc.N_X, mc.N_Y, mc.N_frame)\n",
    "    name = 'static'\n",
    "    env = mc.envelope_gabor(fx, fy, ft, sf_0=sf_0, B_sf=B_sf,\n",
    "                            B_theta=np.inf, V_X=0., V_Y=0., B_V=0, alpha=.5)\n",
    "\n",
    "    z = mc.rectif(mc.random_cloud(env))\n",
    "    z = z.reshape((mc.N_X, mc.N_Y))\n",
    "    return z\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Réseau de neurones"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "# cf 2018-05-17_classification_relu_network\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden1, n_hidden2, n_hidden3, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden1 = torch.nn.Linear(n_feature, n_hidden1)\n",
    "        self.hidden2 = torch.nn.Linear(n_hidden1, n_hidden2)\n",
    "        self.hidden3 = torch.nn.Linear(n_hidden2, n_hidden3)\n",
    "        self.predict = torch.nn.Linear(n_hidden3, n_output)\n",
    "\n",
    "    def forward(self, data):\n",
    "        data = F.leaky_relu(self.hidden1(data))\n",
    "        data = F.leaky_relu(self.hidden2(data))\n",
    "        data = F.leaky_relu(self.hidden3(data))\n",
    "        data = self.predict(data)\n",
    "        data = F.sigmoid(data)\n",
    "        return data\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden1, n_RNN, n_hidden2, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden1 = torch.nn.Linear(n_feature, n_hidden1)\n",
    "        self.RNN = torch.nn.RNN(\n",
    "        n_hidden1, n_RNN, nonlinearity='relu', bias=True, bidirectional=False)\n",
    "        self.hidden2 = torch.nn.Linear(n_RNN, n_hidden1)\n",
    "        self.predict = torch.nn.Linear(n_hidden2, n_output)\n",
    "\n",
    "    def forward(self, data):\n",
    "        data = F.leaky_relu(self.hidden1(data))\n",
    "        #data, weights = self.RNN(data)\n",
    "        data = F.leaky_relu(self.hidden2(data))\n",
    "        data = self.predict(data)\n",
    "        data = F.sigmoid(data)\n",
    "        return data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-23T12:48:13.240171Z",
     "start_time": "2018-05-23T12:48:13.167086Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_size = 100  # quantity of examples that'll be processed\n",
    "lr = 0.05\n",
    "n_hidden1 = 160\n",
    "n_RNN = 90\n",
    "n_hidden2 = 195\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('/tmp/data',\n",
    "                   train=True,  # def the dataset as training data\n",
    "                   download=True,  # download if dataset not present on disk\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))])),\n",
    "    batch_size=sample_size,\n",
    "    shuffle=True)\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden1, n_RNN, n_hidden2, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden1 = torch.nn.Linear(n_feature, n_hidden1)\n",
    "        self.hidden2 = torch.nn.Linear(n_hidden1, n_hidden2)\n",
    "        self.predict = torch.nn.Linear(n_hidden2, n_output)\n",
    "\n",
    "    def forward(self, data):\n",
    "        data = F.leaky_relu(self.hidden1(data))\n",
    "        #data, weights = self.RNN(data)\n",
    "        data = F.leaky_relu(self.hidden2(data))\n",
    "        data = self.predict(data)\n",
    "        return data\n",
    "\n",
    "\n",
    "\n",
    "net = Net(n_feature=N_theta*N_orient*N_scale*N_phase, n_hidden1=n_hidden1, n_RNN=n_RNN, n_hidden2=n_hidden2, n_output=N_orient*N_scale)\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "loss_func = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "def train(sample_size, vsize=N_theta*N_orient*N_scale*N_phase, asize=N_orient*N_scale, offset_std=10, offset_max=25):\n",
    "    t_start = time.time()\n",
    "    print('Starting training...')\n",
    "    for batch_idx, (data, label) in enumerate(data_loader):\n",
    "        input, a_data = np.zeros((sample_size, 1, vsize)), np.zeros(\n",
    "            (sample_size, 1, asize))\n",
    "        target = np.zeros((sample_size, asize))\n",
    "        for idx in range(sample_size):\n",
    "            i_offset, j_offset = int(minmax(\n",
    "                np.random.randn()*offset_std, offset_max)), int(minmax(np.random.randn()*offset_std, offset_max))\n",
    "            input[idx, 0, :], a_data[idx, 0, :] = couples(\n",
    "                data[idx, 0, :], i_offset, j_offset)\n",
    "            target[idx, :] = a_data[idx, 0, :]\n",
    "\n",
    "        input, target = Variable(torch.FloatTensor(\n",
    "            input)), Variable(torch.FloatTensor(a_data))\n",
    "\n",
    "        prediction = net(input)\n",
    "        loss = loss_func(prediction, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Epoch {}: [{}/{}] Loss: {} Time: {:.2f} mn'.format(\n",
    "                epoch, batch_idx*sample_size, len(data_loader.dataset),\n",
    "                loss.data.numpy(), (time.time()-t_start)/60))\n",
    "\n",
    "\n",
    "def eval_sacc(vsize=N_theta*N_orient*N_scale*N_phase, asize=N_orient*N_scale, N_pic=N_X, sacc_lim=5, fovea_size=10, fig_type='cmap'):\n",
    "    for batch_idx, (data, label) in enumerate(data_loader):\n",
    "        i_offset, j_offset = int(\n",
    "            minmax(np.random.randn()*10, 35)), int(minmax(np.random.randn()*10, 35))\n",
    "        print('Stimulus position: ({},{})'.format(i_offset, j_offset))\n",
    "        a_data_in_fovea = False\n",
    "        sacc_count = 0\n",
    "\n",
    "        while not a_data_in_fovea:\n",
    "            input, a_data = np.zeros((1, 1, vsize)), np.zeros((1, 1, asize))\n",
    "            input[0, 0, :], a_data[0, 0, :] = couples(\n",
    "                data[0, 0, :], i_offset, j_offset)\n",
    "            input, a_data = Variable(torch.FloatTensor(\n",
    "                input)), Variable(torch.FloatTensor(a_data))\n",
    "\n",
    "            prediction = net(input)\n",
    "            pred_data = prediction.data.numpy()[-1][-1]\n",
    "\n",
    "            if fig_type == 'cmap':\n",
    "                image = energy_plus @ pred_data\n",
    "                image_reshaped = image.reshape(N_pic, N_pic)\n",
    "\n",
    "                fig, ax = plt.subplots(figsize=(13, 10.725))\n",
    "                cmap = ax.pcolor(np.arange(-(N_pic/2), (N_pic/2)),\n",
    "                                 np.arange(-(N_pic/2), (N_pic/2)), image_reshaped)\n",
    "                fig.colorbar(cmap)\n",
    "                plt.axvline(j_offset, c='k')\n",
    "                plt.axhline(i_offset, c='k')\n",
    "\n",
    "                for i_pred in range(0, N_pic):\n",
    "                    for j_pred in range(0, N_pic):\n",
    "                        if image_reshaped[i_pred][j_pred] == image_reshaped.max():\n",
    "                            i_hat, j_hat = i_pred-(N_pic/2), j_pred-(N_pic/2)\n",
    "                            print('Position prediction: ({},{})'.format(\n",
    "                                i_hat, j_hat))\n",
    "                            if fig_type == 'cmap':\n",
    "                                plt.axvline(j_hat, c='r')\n",
    "                                plt.axhline(i_hat, c='r')\n",
    "                            break\n",
    "\n",
    "                # check if number of saccades is beyond threshold\n",
    "                if sacc_count == sacc_lim:\n",
    "                    print('Stimulus position not found, break')\n",
    "                    break\n",
    "\n",
    "                # saccades\n",
    "                i_offset, j_offset = (i_offset - i_hat), (j_offset - j_hat)\n",
    "                sacc_count += 1\n",
    "                print('Stimulus position after saccade: ({}, {})'.format(\n",
    "                    i_offset, j_offset))\n",
    "\n",
    "                # check if the image position is predicted within the fovea\n",
    "                if i_hat <= (fovea_size/2) and j_hat <= (fovea_size/2):\n",
    "                    if i_hat >= -(fovea_size/2) and j_hat >= -(fovea_size/2):\n",
    "                        a_data_in_fovea = True\n",
    "                        print(\n",
    "                            'a_data predicted in fovea, stopping the saccadic exploration')\n",
    "\n",
    "            if fig_type == 'log':\n",
    "                code = energy_plus @ pred_data #np.ravel(pred_data)\n",
    "                global_energy = energy_vector @ code\n",
    "                #code = phi @ code\n",
    "                #global_energy = (code**2).sum(axis=(0, -1))\n",
    "                global_energy = global_energy.reshape(N_scale, N_orient)\n",
    "\n",
    "\n",
    "                log_r_a_data = 1 + \\\n",
    "                    np.log(np.sqrt(i_offset**2 + j_offset**2) /\n",
    "                           np.sqrt(N_X**2 + N_Y**2) / 2) / 5\n",
    "                if j_offset != 0:\n",
    "                    theta_a_data = np.arctan(-i_offset / j_offset)\n",
    "                else:\n",
    "                    theta_a_data = np.sign(-i_offset) * np.pi/2\n",
    "                print('a_data position (log_r, theta) = ({},{})'.format(\n",
    "                    log_r_a_data, theta_a_data))\n",
    "                log_r, theta = np.meshgrid(np.linspace(\n",
    "                    0, 1, N_scale+1), np.linspace(-np.pi*.625, np.pi*1.375, N_orient+1))\n",
    "\n",
    "                fig, ax = plt.subplots(subplot_kw=dict(projection='polar'))\n",
    "                ax.pcolor(theta, log_r, np.fliplr(global_energy))\n",
    "                ax.plot(theta_a_data, log_r_a_data, 'r+')\n",
    "\n",
    "                for n_orient in range(N_orient):\n",
    "                    for n_scale in range(N_scale):\n",
    "                        if global_energy[n_orient][n_scale] == np.max(global_energy):\n",
    "                            print('Position prediction (orient, scale) = ({},{})'.format(\n",
    "                                n_orient, n_scale))\n",
    "\n",
    "                a_data_in_fovea = True\n",
    "\n",
    "        print('*' * 50)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lancer l'apprentissage ou charger les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-23T12:48:13.271531Z",
     "start_time": "2018-05-23T12:48:13.242558Z"
    }
   },
   "outputs": [],
   "source": [
    "path = '2018-05-23_classification_BCELoss.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-23T12:48:13.793074Z",
     "start_time": "2018-05-23T12:48:13.313716Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '2018-05-23_classification_BCELoss.pt': No such file or directory\n",
      "rm: cannot remove '2018-05-23_classification_BCELoss.pt': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!ls -l {path}\n",
    "!rm {path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-05-23T12:48:06.295Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Starting training...\n",
      "Epoch 0: [0/60000] Loss: 1.2537293434143066 Time: 0.02 mn\n",
      "Epoch 0: [10000/60000] Loss: 0.3869515061378479 Time: 2.45 mn\n",
      "Epoch 0: [20000/60000] Loss: 0.37572363018989563 Time: 4.69 mn\n",
      "Epoch 0: [30000/60000] Loss: 0.3690604567527771 Time: 6.88 mn\n",
      "Epoch 0: [40000/60000] Loss: 0.3652280569076538 Time: 9.17 mn\n",
      "Epoch 0: [50000/60000] Loss: 0.3628096878528595 Time: 11.34 mn\n",
      "Model saved at 2018-05-23_classification_BCELoss.pt\n",
      "Starting training...\n",
      "Epoch 1: [0/60000] Loss: 0.36463475227355957 Time: 0.02 mn\n",
      "Epoch 1: [10000/60000] Loss: 0.3614945411682129 Time: 2.33 mn\n",
      "Epoch 1: [20000/60000] Loss: 0.3610016703605652 Time: 4.53 mn\n",
      "Epoch 1: [30000/60000] Loss: 0.3573167026042938 Time: 6.86 mn\n",
      "Epoch 1: [40000/60000] Loss: 0.35854676365852356 Time: 9.72 mn\n",
      "Epoch 1: [50000/60000] Loss: 0.3585130572319031 Time: 12.52 mn\n",
      "Model saved at 2018-05-23_classification_BCELoss.pt\n",
      "Starting training...\n",
      "Epoch 2: [0/60000] Loss: 0.35912466049194336 Time: 0.03 mn\n",
      "Epoch 2: [10000/60000] Loss: 0.35543936491012573 Time: 2.87 mn\n",
      "Epoch 2: [20000/60000] Loss: 0.35690072178840637 Time: 5.94 mn\n",
      "Epoch 2: [30000/60000] Loss: 0.35602229833602905 Time: 9.07 mn\n",
      "Epoch 2: [40000/60000] Loss: 0.35397398471832275 Time: 11.93 mn\n",
      "Epoch 2: [50000/60000] Loss: 0.35680684447288513 Time: 14.29 mn\n",
      "Model saved at 2018-05-23_classification_BCELoss.pt\n",
      "Starting training...\n",
      "Epoch 3: [0/60000] Loss: 0.35749849677085876 Time: 0.02 mn\n",
      "Epoch 3: [10000/60000] Loss: 0.3542545437812805 Time: 2.45 mn\n",
      "Epoch 3: [20000/60000] Loss: 0.35559752583503723 Time: 4.72 mn\n",
      "Epoch 3: [30000/60000] Loss: 0.3572923243045807 Time: 7.25 mn\n",
      "Epoch 3: [40000/60000] Loss: 0.3544469475746155 Time: 9.48 mn\n",
      "Epoch 3: [50000/60000] Loss: 0.3536164462566376 Time: 11.85 mn\n",
      "Model saved at 2018-05-23_classification_BCELoss.pt\n",
      "Starting training...\n",
      "Epoch 4: [0/60000] Loss: 0.35505151748657227 Time: 0.03 mn\n",
      "Epoch 4: [10000/60000] Loss: 0.35418900847435 Time: 2.51 mn\n",
      "Epoch 4: [20000/60000] Loss: 0.35522013902664185 Time: 4.85 mn\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(path):\n",
    "    net.load_state_dict(torch.load(path))\n",
    "    print('Loading file', path)\n",
    "else:\n",
    "    print('Training model...')\n",
    "    N_epochs = 3\n",
    "    for epoch in range(N_epochs):          #max number of training epochs\n",
    "        train(sample_size)                 #starting the learning\n",
    "        torch.save(net.state_dict(), path) #save the neural network state\n",
    "        print('Model saved at', path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lancer l'évaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-05-23T12:48:06.297Z"
    }
   },
   "outputs": [],
   "source": [
    "for _ in range(1):\n",
    "    eval_sacc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-05-23T12:48:06.299Z"
    }
   },
   "outputs": [],
   "source": [
    "for _ in range(1):\n",
    "    eval_sacc(fig_type='log')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
