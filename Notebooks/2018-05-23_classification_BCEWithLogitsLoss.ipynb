{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From https://pytorch.org/docs/0.3.1/nn.html?highlight=crossentropy#bcewithlogitsloss\n",
    "\n",
    "This loss combines a Sigmoid layer and the BCELoss in one single class. This version is more numerically stable than using a plain Sigmoid followed by a BCELoss as, by combining the operations into one layer, we take advantage of the log-sum-exp trick for numerical stability.\n",
    "\n",
    "The loss can be described as:\n",
    "ℓ(x,y)=L={l1,…,lN}⊤,ln=−wn[tn⋅logσ(xn)+(1−tn)⋅log(1−σ(xn))],\n",
    "\n",
    "where N\n",
    "\n",
    "is the batch size. If reduce is True, then\n",
    "ℓ(x,y)={mean(L),sum(L),ifsize_average=True,ifsize_average=False.\n",
    "\n",
    "This is used for measuring the error of a reconstruction in for example an auto-encoder. Note that the targets t[i] should be numbers between 0 and 1.\n",
    "Parameters:\t\n",
    "\n",
    "    weight (Tensor, optional) – a manual rescaling weight given to the loss of each batch element. If given, has to be a Tensor of size “nbatch”.\n",
    "    size_average – By default, the losses are averaged over observations for each minibatch. However, if the field size_average is set to False, the losses are instead summed for each minibatch. Default: True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-23T12:48:07.719392Z",
     "start_time": "2018-05-23T12:48:06.321600Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "#import noise\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import MotionClouds as mc\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from LogGabor import LogGabor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charger la matrice de certitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-23T12:48:07.798260Z",
     "start_time": "2018-05-23T12:48:07.759455Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading accuracy...\n",
      "[[0.0974 0.0974 0.0974 ... 0.0974 0.0974 0.0974]\n",
      " [0.0974 0.0974 0.0974 ... 0.0974 0.0974 0.0974]\n",
      " [0.0974 0.0974 0.0974 ... 0.0974 0.0974 0.0974]\n",
      " ...\n",
      " [0.0974 0.0974 0.0974 ... 0.0974 0.0974 0.0974]\n",
      " [0.0974 0.0974 0.0974 ... 0.0974 0.0974 0.0974]\n",
      " [0.0974 0.0974 0.0974 ... 0.0974 0.0974 0.0974]]\n"
     ]
    }
   ],
   "source": [
    "path = \"MNIST_accuracy.npy\"\n",
    "if os.path.isfile(path):\n",
    "    print('Loading accuracy...')\n",
    "    accuracy =  np.load(path)\n",
    "    print(accuracy)\n",
    "else:\n",
    "    print('No accuracy data found.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Préparer l'apprentissage et les fonctions nécessaires au fonctionnement du script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-23T12:48:13.146359Z",
     "start_time": "2018-05-23T12:48:07.807303Z"
    }
   },
   "outputs": [],
   "source": [
    "N_theta, N_orient, N_scale, N_phase, N_X, N_Y, rho = 6, 12, 5, 2, 128, 128, 1.61803\n",
    "\n",
    "def vectorization(N_theta, N_orient, N_scale, N_phase, N_X, N_Y, rho):\n",
    "    phi = np.zeros((N_theta, N_orient, N_scale, N_phase, N_X*N_Y))\n",
    "    parameterfile = 'https://raw.githubusercontent.com/bicv/LogGabor/master/default_param.py'\n",
    "    lg = LogGabor(parameterfile)\n",
    "    lg.set_size((N_X, N_Y))\n",
    "    params = {'sf_0': .1, 'B_sf': 2*lg.pe.B_sf,\n",
    "              'theta': np.pi * 5 / 7., 'B_theta': 2*lg.pe.B_theta}\n",
    "    phase = np.pi/4\n",
    "    edge = lg.normalize(lg.invert(lg.loggabor(\n",
    "        N_X/3, 3*N_Y/4, **params)*np.exp(-1j*phase)))\n",
    "\n",
    "    for i_theta in range(N_theta):\n",
    "        for i_orient in range(N_orient):\n",
    "            for i_scale in range(N_scale):\n",
    "                ecc = (1/rho)**(N_scale - i_scale)\n",
    "                r = np.sqrt(N_X**2+N_Y**2) / 2 * ecc  # radius\n",
    "                sf_0 = 0.5 * 0.03 / ecc\n",
    "                x = N_X/2 + r * \\\n",
    "                    np.cos((i_orient+(i_scale % 2)*.5)*np.pi*2 / N_orient)\n",
    "                y = N_Y/2 + r * \\\n",
    "                    np.sin((i_orient+(i_scale % 2)*.5)*np.pi*2 / N_orient)\n",
    "                for i_phase in range(N_phase):\n",
    "                    params = {'sf_0': sf_0, 'B_sf': lg.pe.B_sf,\n",
    "                              'theta': i_theta*np.pi/N_theta, 'B_theta': np.pi/N_theta/2}\n",
    "                    phase = i_phase * np.pi/2\n",
    "                    phi[i_theta, i_orient, i_scale, i_phase, :] = lg.normalize(\n",
    "                        lg.invert(lg.loggabor(x, y, **params)*np.exp(-1j*phase))).ravel()\n",
    "    return phi\n",
    "\n",
    "\n",
    "phi = vectorization(N_theta, N_orient, N_scale, N_phase, N_X, N_Y, rho)\n",
    "phi_vector = phi.reshape((N_theta*N_orient*N_scale*N_phase, N_X*N_Y))\n",
    "phi_plus = np.linalg.pinv(phi_vector)\n",
    "\n",
    "energy = (phi**2).sum(axis=(0, 3))\n",
    "energy /= energy.sum(axis=-1)[:, :, None]\n",
    "energy_vector = energy.reshape((N_orient*N_scale, N_X*N_Y))\n",
    "energy_plus = np.linalg.pinv(energy_vector)\n",
    "\n",
    "def accuracy_128(i_offset, j_offset, N_pic=128, N_stim=55):\n",
    "    center = (N_pic-N_stim)//2\n",
    "\n",
    "    accuracy_128 = 0.1 * np.ones((N_pic, N_pic))\n",
    "    accuracy_128[(center+i_offset):(center+N_stim+i_offset),\n",
    "                 (center+j_offset):(center+N_stim+j_offset)] = accuracy\n",
    "\n",
    "    accuracy_LP = energy_vector @ np.ravel(accuracy_128)\n",
    "    return accuracy_LP\n",
    "\n",
    "\n",
    "def mnist_128(data, i_offset, j_offset, N_pic=128, N_stim=28, noise=True, noise_type='MotionCloud'):\n",
    "    center = (N_pic-N_stim)//2\n",
    "    #data_128 = np.zeros((N_pic, N_pic))\n",
    "    data_128 = (data.min().numpy()) * np.ones((N_pic, N_pic))\n",
    "\n",
    "    data_128[int(center+i_offset):int(center+N_stim+i_offset),\n",
    "             int(center+j_offset):int(center+N_stim+j_offset)] = data\n",
    "\n",
    "    if noise:\n",
    "        if noise_type == 'MotionCloud':\n",
    "            data_LP = phi_vector @ np.ravel(data_128 + MotionCloudNoise())\n",
    "        elif noise_type == 'Perlin':\n",
    "            data_LP = phi_vector @ np.ravel(\n",
    "                data_128 + randomized_perlin_noise())\n",
    "    else:\n",
    "        data_LP = phi_vector @ np.ravel(data_128)\n",
    "    return data_LP\n",
    "\n",
    "\n",
    "def couples(data, i_offset, j_offset, device):\n",
    "    data = data.to(device)\n",
    "    v = mnist_128(data.cpu(), i_offset, j_offset)\n",
    "    a = accuracy_128(i_offset, j_offset)\n",
    "    return (v, a)\n",
    "\n",
    "\n",
    "def minmax(value, border):\n",
    "    value = max(value, -border)\n",
    "    value = min(value, border)\n",
    "    return value\n",
    "\n",
    "\n",
    "def sigmoid(values):\n",
    "    values = 1 / (1 + ((1 / 0.1) - 1) * np.exp(-values))\n",
    "    return values\n",
    "\n",
    "\n",
    "def randomized_perlin_noise(shape=(128, 128), scale=10, octaves=6, persistence=0.5, lacunarity=2.0, base=0):\n",
    "    noise_vector = np.zeros(shape)\n",
    "    for i in range(shape[0]):\n",
    "        for j in range(shape[1]):\n",
    "            noise_vector[i][j] = noise.pnoise2(i/scale,\n",
    "                                               j/scale,\n",
    "                                               octaves=int(\n",
    "                                                   octave * abs(np.random.randn()))+1,\n",
    "                                               persistence=persistence *\n",
    "                                               abs(np.random.randn()),\n",
    "                                               lacunarity=lacunarity *\n",
    "                                               abs(np.random.randn()),\n",
    "                                               repeatx=shape[0],\n",
    "                                               repeaty=shape[1],\n",
    "                                               base=base)\n",
    "    return noise_vector\n",
    "\n",
    "\n",
    "def MotionCloudNoise(sf_0=0.125, B_sf=3.):\n",
    "    mc.N_X, mc.N_Y, mc.N_frame = 128, 128, 1\n",
    "    fx, fy, ft = mc.get_grids(mc.N_X, mc.N_Y, mc.N_frame)\n",
    "    name = 'static'\n",
    "    env = mc.envelope_gabor(fx, fy, ft, sf_0=sf_0, B_sf=B_sf,\n",
    "                            B_theta=np.inf, V_X=0., V_Y=0., B_V=0, alpha=.5)\n",
    "\n",
    "    z = mc.rectif(mc.random_cloud(env))\n",
    "    z = z.reshape((mc.N_X, mc.N_Y))\n",
    "    return z\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Réseau de neurones"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "# cf 2018-05-17_classification_relu_network\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden1, n_hidden2, n_hidden3, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden1 = torch.nn.Linear(n_feature, n_hidden1)\n",
    "        self.hidden2 = torch.nn.Linear(n_hidden1, n_hidden2)\n",
    "        self.hidden3 = torch.nn.Linear(n_hidden2, n_hidden3)\n",
    "        self.predict = torch.nn.Linear(n_hidden3, n_output)\n",
    "\n",
    "    def forward(self, data):\n",
    "        data = F.leaky_relu(self.hidden1(data))\n",
    "        data = F.leaky_relu(self.hidden2(data))\n",
    "        data = F.leaky_relu(self.hidden3(data))\n",
    "        data = self.predict(data)\n",
    "        data = F.sigmoid(data)\n",
    "        return data\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden1, n_RNN, n_hidden2, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden1 = torch.nn.Linear(n_feature, n_hidden1)\n",
    "        self.RNN = torch.nn.RNN(\n",
    "        n_hidden1, n_RNN, nonlinearity='relu', bias=True, bidirectional=False)\n",
    "        self.hidden2 = torch.nn.Linear(n_RNN, n_hidden1)\n",
    "        self.predict = torch.nn.Linear(n_hidden2, n_output)\n",
    "\n",
    "    def forward(self, data):\n",
    "        data = F.leaky_relu(self.hidden1(data))\n",
    "        #data, weights = self.RNN(data)\n",
    "        data = F.leaky_relu(self.hidden2(data))\n",
    "        data = self.predict(data)\n",
    "        data = F.sigmoid(data)\n",
    "        return data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-23T12:48:13.240171Z",
     "start_time": "2018-05-23T12:48:13.167086Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_size = 100  # quantity of examples that'll be processed\n",
    "lr = 0.05\n",
    "n_hidden1 = 160\n",
    "n_RNN = 90\n",
    "n_hidden2 = 195\n",
    "\n",
    "do_cuda = torch.cuda.is_available()\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if do_cuda else {}\n",
    "device = torch.device(\"cuda\" if do_cuda else \"cpu\")\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('/tmp/data',\n",
    "                   train=True,  # def the dataset as training data\n",
    "                   download=True,  # download if dataset not present on disk\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))])),\n",
    "    batch_size=sample_size,\n",
    "    shuffle=True, **kwargs)\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden1, n_RNN, n_hidden2, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden1 = torch.nn.Linear(n_feature, n_hidden1)\n",
    "        self.hidden2 = torch.nn.Linear(n_hidden1, n_hidden2)\n",
    "        self.predict = torch.nn.Linear(n_hidden2, n_output)\n",
    "\n",
    "    def forward(self, data):\n",
    "        data = F.leaky_relu(self.hidden1(data))\n",
    "        #data, weights = self.RNN(data)\n",
    "        data = F.leaky_relu(self.hidden2(data))\n",
    "        data = self.predict(data)\n",
    "        return data\n",
    "\n",
    "\n",
    "net = Net(n_feature=N_theta*N_orient*N_scale*N_phase, n_hidden1=n_hidden1, n_RNN=n_RNN, n_hidden2=n_hidden2, n_output=N_orient*N_scale).to(device)\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "loss_func = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "    \n",
    "def train(sample_size, optimizer=optimizer, vsize=N_theta*N_orient*N_scale*N_phase, asize=N_orient*N_scale, offset_std=10, offset_max=25, verbose=1):\n",
    "    t_start = time.time()\n",
    "    if verbose: print('Starting training...')\n",
    "    for batch_idx, (data, label) in enumerate(data_loader):\n",
    "        input, a_data = np.zeros((sample_size, 1, vsize)), np.zeros(\n",
    "            (sample_size, 1, asize))\n",
    "        target = np.zeros((sample_size, asize))\n",
    "        for idx in range(sample_size):\n",
    "            i_offset, j_offset = int(minmax(\n",
    "                np.random.randn()*offset_std, offset_max)), int(minmax(np.random.randn()*offset_std, offset_max))\n",
    "            input[idx, 0, :], a_data[idx, 0, :] = couples(\n",
    "                data[idx, 0, :], i_offset, j_offset)\n",
    "            target[idx, :] = a_data[idx, 0, :]\n",
    "\n",
    "        input, target = Variable(torch.FloatTensor(\n",
    "            input)), Variable(torch.FloatTensor(a_data))\n",
    "\n",
    "        prediction = net(input)\n",
    "        loss = loss_func(prediction, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if verbose and batch_idx % 100 == 0:\n",
    "            print('Epoch {}: [{}/{}] Loss: {} Time: {:.2f} mn'.format(\n",
    "                epoch, batch_idx*sample_size, len(data_loader.dataset),\n",
    "                loss.data.numpy(), (time.time()-t_start)/60))\n",
    "        return loss.data.numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def eval_sacc(vsize=N_theta*N_orient*N_scale*N_phase, asize=N_orient*N_scale, N_pic=N_X, sacc_lim=5, fovea_size=10, fig_type='cmap'):\n",
    "    for batch_idx, (data, label) in enumerate(data_loader):\n",
    "        data = data.to(device)\n",
    "        i_offset, j_offset = int(\n",
    "            minmax(np.random.randn()*10, 35)), int(minmax(np.random.randn()*10, 35))\n",
    "        print('Stimulus position: ({},{})'.format(i_offset, j_offset))\n",
    "        a_data_in_fovea = False\n",
    "        sacc_count = 0\n",
    "\n",
    "        while not a_data_in_fovea:\n",
    "            input, a_data = np.zeros((1, 1, vsize)), np.zeros((1, 1, asize))\n",
    "            input[0, 0, :], a_data[0, 0, :] = couples(\n",
    "                data[0, 0, :], i_offset, j_offset)\n",
    "            input, a_data = Variable(torch.FloatTensor(\n",
    "                input)), Variable(torch.FloatTensor(a_data))\n",
    "\n",
    "            prediction = net(input)\n",
    "            pred_data = prediction.data.numpy()[-1][-1]\n",
    "\n",
    "            if fig_type == 'cmap':\n",
    "                image = energy_plus @ pred_data\n",
    "                image_reshaped = image.reshape(N_pic, N_pic)\n",
    "\n",
    "                fig, ax = plt.subplots(figsize=(13, 10.725))\n",
    "                cmap = ax.pcolor(np.arange(-(N_pic/2), (N_pic/2)),\n",
    "                                 np.arange(-(N_pic/2), (N_pic/2)), image_reshaped)\n",
    "                fig.colorbar(cmap)\n",
    "                plt.axvline(j_offset, c='k')\n",
    "                plt.axhline(i_offset, c='k')\n",
    "\n",
    "                for i_pred in range(0, N_pic):\n",
    "                    for j_pred in range(0, N_pic):\n",
    "                        if image_reshaped[i_pred][j_pred] == image_reshaped.max():\n",
    "                            i_hat, j_hat = i_pred-(N_pic/2), j_pred-(N_pic/2)\n",
    "                            print('Position prediction: ({},{})'.format(\n",
    "                                i_hat, j_hat))\n",
    "                            if fig_type == 'cmap':\n",
    "                                plt.axvline(j_hat, c='r')\n",
    "                                plt.axhline(i_hat, c='r')\n",
    "                            break\n",
    "\n",
    "                # check if number of saccades is beyond threshold\n",
    "                if sacc_count == sacc_lim:\n",
    "                    print('Stimulus position not found, break')\n",
    "                    break\n",
    "\n",
    "                # saccades\n",
    "                i_offset, j_offset = (i_offset - i_hat), (j_offset - j_hat)\n",
    "                sacc_count += 1\n",
    "                print('Stimulus position after saccade: ({}, {})'.format(\n",
    "                    i_offset, j_offset))\n",
    "\n",
    "                # check if the image position is predicted within the fovea\n",
    "                if i_hat <= (fovea_size/2) and j_hat <= (fovea_size/2):\n",
    "                    if i_hat >= -(fovea_size/2) and j_hat >= -(fovea_size/2):\n",
    "                        a_data_in_fovea = True\n",
    "                        print(\n",
    "                            'a_data predicted in fovea, stopping the saccadic exploration')\n",
    "\n",
    "            if fig_type == 'log':\n",
    "                code = energy_plus @ pred_data #np.ravel(pred_data)\n",
    "                global_energy = energy_vector @ code\n",
    "                #code = phi @ code\n",
    "                #global_energy = (code**2).sum(axis=(0, -1))\n",
    "                global_energy = global_energy.reshape(N_scale, N_orient)\n",
    "\n",
    "\n",
    "                log_r_a_data = 1 + \\\n",
    "                    np.log(np.sqrt(i_offset**2 + j_offset**2) /\n",
    "                           np.sqrt(N_X**2 + N_Y**2) / 2) / 5\n",
    "                if j_offset != 0:\n",
    "                    theta_a_data = np.arctan(-i_offset / j_offset)\n",
    "                else:\n",
    "                    theta_a_data = np.sign(-i_offset) * np.pi/2\n",
    "                print('a_data position (log_r, theta) = ({},{})'.format(\n",
    "                    log_r_a_data, theta_a_data))\n",
    "                log_r, theta = np.meshgrid(np.linspace(\n",
    "                    0, 1, N_scale+1), np.linspace(-np.pi*.625, np.pi*1.375, N_orient+1))\n",
    "\n",
    "                fig, ax = plt.subplots(subplot_kw=dict(projection='polar'))\n",
    "                ax.pcolor(theta, log_r, np.fliplr(global_energy))\n",
    "                ax.plot(theta_a_data, log_r_a_data, 'r+')\n",
    "\n",
    "                for n_orient in range(N_orient):\n",
    "                    for n_scale in range(N_scale):\n",
    "                        if global_energy[n_orient][n_scale] == np.max(global_energy):\n",
    "                            print('Position prediction (orient, scale) = ({},{})'.format(\n",
    "                                n_orient, n_scale))\n",
    "\n",
    "                a_data_in_fovea = True\n",
    "\n",
    "        print('*' * 50)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lancer l'apprentissage ou charger les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-23T12:48:13.271531Z",
     "start_time": "2018-05-23T12:48:13.242558Z"
    }
   },
   "outputs": [],
   "source": [
    "path = '2018-05-23_classification_BCELoss.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-23T12:48:13.793074Z",
     "start_time": "2018-05-23T12:48:13.313716Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '2018-05-23_classification_BCELoss.pt': No such file or directory\n",
      "rm: cannot remove '2018-05-23_classification_BCELoss.pt': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!ls -l {path}\n",
    "#!rm {path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-05-23T12:48:06.295Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model... with lr= 0.005000000000000001\n",
      "Starting training...\n",
      "Epoch 0: [0/60000] Loss: 1.1467702388763428 Time: 0.03 mn\n",
      "Epoch 0: [10000/60000] Loss: 0.45408013463020325 Time: 3.73 mn\n",
      "Epoch 0: [20000/60000] Loss: 0.42948466539382935 Time: 8.55 mn\n",
      "Epoch 0: [30000/60000] Loss: 0.4156387150287628 Time: 13.35 mn\n",
      "Epoch 0: [40000/60000] Loss: 0.410631388425827 Time: 18.30 mn\n",
      "Epoch 0: [50000/60000] Loss: 0.405883252620697 Time: 22.57 mn\n",
      "Starting training...\n",
      "Epoch 1: [0/60000] Loss: 0.39986830949783325 Time: 0.05 mn\n",
      "Epoch 1: [10000/60000] Loss: 0.39864519238471985 Time: 4.78 mn\n",
      "Epoch 1: [20000/60000] Loss: 0.39849162101745605 Time: 9.73 mn\n",
      "Epoch 1: [30000/60000] Loss: 0.391185998916626 Time: 14.68 mn\n",
      "Epoch 1: [40000/60000] Loss: 0.3888004422187805 Time: 19.58 mn\n",
      "Epoch 1: [50000/60000] Loss: 0.38680970668792725 Time: 24.38 mn\n",
      "Starting training...\n",
      "Epoch 2: [0/60000] Loss: 0.386436402797699 Time: 0.05 mn\n",
      "Epoch 2: [10000/60000] Loss: 0.38260969519615173 Time: 4.48 mn\n",
      "Epoch 2: [20000/60000] Loss: 0.38431328535079956 Time: 9.35 mn\n",
      "Epoch 2: [30000/60000] Loss: 0.38067030906677246 Time: 14.26 mn\n",
      "Epoch 2: [40000/60000] Loss: 0.38253456354141235 Time: 18.92 mn\n",
      "Epoch 2: [50000/60000] Loss: 0.37865716218948364 Time: 22.93 mn\n",
      "Starting training...\n",
      "Epoch 3: [0/60000] Loss: 0.37665700912475586 Time: 0.04 mn\n",
      "Epoch 3: [10000/60000] Loss: 0.3766082227230072 Time: 4.92 mn\n",
      "Epoch 3: [20000/60000] Loss: 0.3774071931838989 Time: 9.53 mn\n",
      "Epoch 3: [30000/60000] Loss: 0.37459665536880493 Time: 13.52 mn\n",
      "Epoch 3: [40000/60000] Loss: 0.3708457052707672 Time: 17.46 mn\n",
      "Epoch 3: [50000/60000] Loss: 0.3725389242172241 Time: 21.44 mn\n",
      "Starting training...\n",
      "Epoch 4: [0/60000] Loss: 0.3712720572948456 Time: 0.04 mn\n",
      "Epoch 4: [10000/60000] Loss: 0.37278351187705994 Time: 3.97 mn\n",
      "Epoch 4: [20000/60000] Loss: 0.37239646911621094 Time: 7.87 mn\n",
      "Epoch 4: [30000/60000] Loss: 0.37246742844581604 Time: 11.83 mn\n",
      "Epoch 4: [40000/60000] Loss: 0.36897793412208557 Time: 15.74 mn\n",
      "Epoch 4: [50000/60000] Loss: 0.3725430965423584 Time: 19.67 mn\n",
      "Starting training...\n",
      "Epoch 5: [0/60000] Loss: 0.37161675095558167 Time: 0.04 mn\n",
      "Epoch 5: [10000/60000] Loss: 0.36943429708480835 Time: 3.97 mn\n",
      "Epoch 5: [20000/60000] Loss: 0.36944717168807983 Time: 7.91 mn\n",
      "Epoch 5: [30000/60000] Loss: 0.370399534702301 Time: 11.87 mn\n",
      "Epoch 5: [40000/60000] Loss: 0.3690184950828552 Time: 15.81 mn\n",
      "Epoch 5: [50000/60000] Loss: 0.36910536885261536 Time: 19.71 mn\n",
      "Starting training...\n",
      "Epoch 6: [0/60000] Loss: 0.3668314218521118 Time: 0.04 mn\n",
      "Epoch 6: [10000/60000] Loss: 0.36715224385261536 Time: 4.02 mn\n",
      "Epoch 6: [20000/60000] Loss: 0.36708295345306396 Time: 7.97 mn\n",
      "Epoch 6: [30000/60000] Loss: 0.3688944876194 Time: 11.87 mn\n",
      "Epoch 6: [40000/60000] Loss: 0.36482781171798706 Time: 15.79 mn\n",
      "Epoch 6: [50000/60000] Loss: 0.3662613332271576 Time: 19.72 mn\n",
      "Starting training...\n",
      "Epoch 7: [0/60000] Loss: 0.3674766421318054 Time: 0.04 mn\n",
      "Epoch 7: [10000/60000] Loss: 0.3641296327114105 Time: 3.98 mn\n",
      "Epoch 7: [20000/60000] Loss: 0.36521241068840027 Time: 7.92 mn\n",
      "Epoch 7: [30000/60000] Loss: 0.36627286672592163 Time: 11.88 mn\n",
      "Epoch 7: [40000/60000] Loss: 0.36683720350265503 Time: 15.80 mn\n",
      "Epoch 7: [50000/60000] Loss: 0.36472827196121216 Time: 19.78 mn\n",
      "Starting training...\n",
      "Epoch 8: [0/60000] Loss: 0.36547696590423584 Time: 0.04 mn\n",
      "Epoch 8: [10000/60000] Loss: 0.3628098964691162 Time: 4.00 mn\n",
      "Epoch 8: [20000/60000] Loss: 0.36624962091445923 Time: 7.94 mn\n",
      "Epoch 8: [30000/60000] Loss: 0.3657486140727997 Time: 11.89 mn\n",
      "Epoch 8: [40000/60000] Loss: 0.36232736706733704 Time: 15.83 mn\n",
      "Epoch 8: [50000/60000] Loss: 0.3613649606704712 Time: 19.77 mn\n",
      "Starting training...\n",
      "Epoch 9: [0/60000] Loss: 0.3632926046848297 Time: 0.04 mn\n",
      "Epoch 9: [10000/60000] Loss: 0.3627030849456787 Time: 3.96 mn\n",
      "Epoch 9: [20000/60000] Loss: 0.3637496531009674 Time: 7.90 mn\n",
      "Epoch 9: [30000/60000] Loss: 0.3644718825817108 Time: 11.87 mn\n",
      "Epoch 9: [40000/60000] Loss: 0.3635690212249756 Time: 15.83 mn\n",
      "Epoch 9: [50000/60000] Loss: 0.36202073097229004 Time: 19.76 mn\n",
      "Training model... with lr= 0.008891397050194615\n",
      "Starting training...\n",
      "Epoch 0: [0/60000] Loss: 0.987529993057251 Time: 0.05 mn\n",
      "Epoch 0: [10000/60000] Loss: 0.4347180724143982 Time: 3.93 mn\n",
      "Epoch 0: [20000/60000] Loss: 0.4184166491031647 Time: 7.91 mn\n",
      "Epoch 0: [30000/60000] Loss: 0.4047214984893799 Time: 11.88 mn\n",
      "Epoch 0: [40000/60000] Loss: 0.396280437707901 Time: 15.81 mn\n",
      "Epoch 0: [50000/60000] Loss: 0.38993653655052185 Time: 19.72 mn\n",
      "Starting training...\n",
      "Epoch 1: [0/60000] Loss: 0.388386994600296 Time: 0.04 mn\n",
      "Epoch 1: [10000/60000] Loss: 0.3862445652484894 Time: 3.98 mn\n",
      "Epoch 1: [20000/60000] Loss: 0.38295385241508484 Time: 7.96 mn\n",
      "Epoch 1: [30000/60000] Loss: 0.3801889419555664 Time: 11.91 mn\n",
      "Epoch 1: [40000/60000] Loss: 0.3797079920768738 Time: 15.82 mn\n",
      "Epoch 1: [50000/60000] Loss: 0.3753472566604614 Time: 19.75 mn\n",
      "Starting training...\n",
      "Epoch 2: [0/60000] Loss: 0.3765569031238556 Time: 0.04 mn\n",
      "Epoch 2: [10000/60000] Loss: 0.37387600541114807 Time: 3.97 mn\n",
      "Epoch 2: [20000/60000] Loss: 0.37110504508018494 Time: 7.91 mn\n",
      "Epoch 2: [30000/60000] Loss: 0.3728165924549103 Time: 11.85 mn\n",
      "Epoch 2: [40000/60000] Loss: 0.3720349967479706 Time: 15.78 mn\n",
      "Epoch 2: [50000/60000] Loss: 0.37015143036842346 Time: 19.72 mn\n",
      "Starting training...\n",
      "Epoch 3: [0/60000] Loss: 0.3707852065563202 Time: 0.04 mn\n",
      "Epoch 3: [10000/60000] Loss: 0.3679279685020447 Time: 4.05 mn\n",
      "Epoch 3: [20000/60000] Loss: 0.3686266839504242 Time: 8.02 mn\n",
      "Epoch 3: [30000/60000] Loss: 0.3678470551967621 Time: 11.96 mn\n",
      "Epoch 3: [40000/60000] Loss: 0.366249680519104 Time: 15.89 mn\n",
      "Epoch 3: [50000/60000] Loss: 0.3675006031990051 Time: 19.84 mn\n",
      "Starting training...\n",
      "Epoch 4: [0/60000] Loss: 0.36736032366752625 Time: 0.04 mn\n",
      "Epoch 4: [10000/60000] Loss: 0.3655394911766052 Time: 4.02 mn\n",
      "Epoch 4: [20000/60000] Loss: 0.3669498860836029 Time: 7.95 mn\n",
      "Epoch 4: [30000/60000] Loss: 0.36699071526527405 Time: 11.88 mn\n",
      "Epoch 4: [40000/60000] Loss: 0.36319833993911743 Time: 15.87 mn\n",
      "Epoch 4: [50000/60000] Loss: 0.36626896262168884 Time: 19.81 mn\n",
      "Starting training...\n",
      "Epoch 5: [0/60000] Loss: 0.3650844693183899 Time: 0.04 mn\n",
      "Epoch 5: [10000/60000] Loss: 0.3639336824417114 Time: 3.96 mn\n",
      "Epoch 5: [20000/60000] Loss: 0.36412912607192993 Time: 7.91 mn\n",
      "Epoch 5: [30000/60000] Loss: 0.36340463161468506 Time: 11.85 mn\n",
      "Epoch 5: [40000/60000] Loss: 0.3640769422054291 Time: 15.80 mn\n",
      "Epoch 5: [50000/60000] Loss: 0.3633717894554138 Time: 19.75 mn\n",
      "Starting training...\n",
      "Epoch 6: [0/60000] Loss: 0.3639377951622009 Time: 0.04 mn\n",
      "Epoch 6: [10000/60000] Loss: 0.36239999532699585 Time: 3.95 mn\n",
      "Epoch 6: [20000/60000] Loss: 0.36265867948532104 Time: 7.88 mn\n",
      "Epoch 6: [30000/60000] Loss: 0.36380138993263245 Time: 11.82 mn\n",
      "Epoch 6: [40000/60000] Loss: 0.36110803484916687 Time: 15.72 mn\n",
      "Epoch 6: [50000/60000] Loss: 0.3628806471824646 Time: 19.68 mn\n",
      "Starting training...\n",
      "Epoch 7: [0/60000] Loss: 0.362871915102005 Time: 0.04 mn\n",
      "Epoch 7: [10000/60000] Loss: 0.36168283224105835 Time: 3.96 mn\n",
      "Epoch 7: [20000/60000] Loss: 0.3609916567802429 Time: 7.90 mn\n",
      "Epoch 7: [30000/60000] Loss: 0.3618095815181732 Time: 11.84 mn\n",
      "Epoch 7: [40000/60000] Loss: 0.35911333560943604 Time: 15.78 mn\n",
      "Epoch 7: [50000/60000] Loss: 0.35758116841316223 Time: 19.72 mn\n",
      "Starting training...\n",
      "Epoch 8: [0/60000] Loss: 0.3600454330444336 Time: 0.04 mn\n",
      "Epoch 8: [10000/60000] Loss: 0.35957053303718567 Time: 4.00 mn\n",
      "Epoch 8: [20000/60000] Loss: 0.3598692715167999 Time: 7.95 mn\n",
      "Epoch 8: [30000/60000] Loss: 0.35991933941841125 Time: 11.90 mn\n",
      "Epoch 8: [40000/60000] Loss: 0.35984811186790466 Time: 15.83 mn\n",
      "Epoch 8: [50000/60000] Loss: 0.3622121810913086 Time: 19.74 mn\n",
      "Starting training...\n",
      "Epoch 9: [0/60000] Loss: 0.3589887022972107 Time: 0.04 mn\n",
      "Epoch 9: [10000/60000] Loss: 0.3581944704055786 Time: 3.98 mn\n",
      "Epoch 9: [20000/60000] Loss: 0.35830143094062805 Time: 7.92 mn\n",
      "Epoch 9: [30000/60000] Loss: 0.35970163345336914 Time: 11.89 mn\n",
      "Epoch 9: [40000/60000] Loss: 0.35794296860694885 Time: 15.86 mn\n",
      "Epoch 9: [50000/60000] Loss: 0.3591330647468567 Time: 19.80 mn\n",
      "Training model... with lr= 0.0158113883008419\n",
      "Starting training...\n",
      "Epoch 0: [0/60000] Loss: 1.3584927320480347 Time: 0.04 mn\n",
      "Epoch 0: [10000/60000] Loss: 0.4198076128959656 Time: 3.98 mn\n",
      "Epoch 0: [20000/60000] Loss: 0.39743196964263916 Time: 7.92 mn\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: [30000/60000] Loss: 0.389565110206604 Time: 11.84 mn\n",
      "Epoch 0: [40000/60000] Loss: 0.386134535074234 Time: 15.79 mn\n",
      "Epoch 0: [50000/60000] Loss: 0.37884601950645447 Time: 19.70 mn\n",
      "Starting training...\n",
      "Epoch 1: [0/60000] Loss: 0.37659141421318054 Time: 0.04 mn\n",
      "Epoch 1: [10000/60000] Loss: 0.3744199573993683 Time: 3.96 mn\n",
      "Epoch 1: [20000/60000] Loss: 0.37413302063941956 Time: 7.91 mn\n",
      "Epoch 1: [30000/60000] Loss: 0.37078383564949036 Time: 11.88 mn\n",
      "Epoch 1: [40000/60000] Loss: 0.3702322542667389 Time: 15.82 mn\n",
      "Epoch 1: [50000/60000] Loss: 0.3680552542209625 Time: 19.76 mn\n",
      "Starting training...\n",
      "Epoch 2: [0/60000] Loss: 0.3683009743690491 Time: 0.04 mn\n",
      "Epoch 2: [10000/60000] Loss: 0.36888304352760315 Time: 3.94 mn\n",
      "Epoch 2: [20000/60000] Loss: 0.3656168282032013 Time: 7.87 mn\n",
      "Epoch 2: [30000/60000] Loss: 0.36559054255485535 Time: 11.81 mn\n",
      "Epoch 2: [40000/60000] Loss: 0.36421146988868713 Time: 15.72 mn\n",
      "Epoch 2: [50000/60000] Loss: 0.3648037910461426 Time: 19.65 mn\n",
      "Starting training...\n",
      "Epoch 3: [0/60000] Loss: 0.3631567060947418 Time: 0.04 mn\n",
      "Epoch 3: [10000/60000] Loss: 0.36414802074432373 Time: 3.96 mn\n",
      "Epoch 3: [20000/60000] Loss: 0.3648167848587036 Time: 7.85 mn\n",
      "Epoch 3: [30000/60000] Loss: 0.36427879333496094 Time: 11.81 mn\n",
      "Epoch 3: [40000/60000] Loss: 0.362560510635376 Time: 15.74 mn\n",
      "Epoch 3: [50000/60000] Loss: 0.3609413802623749 Time: 19.69 mn\n",
      "Starting training...\n",
      "Epoch 4: [0/60000] Loss: 0.36078327894210815 Time: 0.04 mn\n",
      "Epoch 4: [10000/60000] Loss: 0.36125680804252625 Time: 3.96 mn\n",
      "Epoch 4: [20000/60000] Loss: 0.35912275314331055 Time: 7.87 mn\n",
      "Epoch 4: [30000/60000] Loss: 0.35896748304367065 Time: 11.82 mn\n",
      "Epoch 4: [40000/60000] Loss: 0.3613488972187042 Time: 15.76 mn\n",
      "Epoch 4: [50000/60000] Loss: 0.35810282826423645 Time: 19.69 mn\n",
      "Starting training...\n",
      "Epoch 5: [0/60000] Loss: 0.35924389958381653 Time: 0.04 mn\n",
      "Epoch 5: [10000/60000] Loss: 0.3594039976596832 Time: 3.95 mn\n",
      "Epoch 5: [20000/60000] Loss: 0.3587838411331177 Time: 7.88 mn\n",
      "Epoch 5: [30000/60000] Loss: 0.3582581579685211 Time: 11.82 mn\n",
      "Epoch 5: [40000/60000] Loss: 0.3581286072731018 Time: 15.75 mn\n",
      "Epoch 5: [50000/60000] Loss: 0.3594592213630676 Time: 19.66 mn\n",
      "Starting training...\n",
      "Epoch 6: [0/60000] Loss: 0.3578754961490631 Time: 0.04 mn\n",
      "Epoch 6: [10000/60000] Loss: 0.3595307171344757 Time: 3.97 mn\n",
      "Epoch 6: [20000/60000] Loss: 0.3580169975757599 Time: 7.91 mn\n",
      "Epoch 6: [30000/60000] Loss: 0.35657230019569397 Time: 11.84 mn\n",
      "Epoch 6: [40000/60000] Loss: 0.35809481143951416 Time: 15.78 mn\n",
      "Epoch 6: [50000/60000] Loss: 0.3572889566421509 Time: 19.74 mn\n",
      "Starting training...\n",
      "Epoch 7: [0/60000] Loss: 0.3567471206188202 Time: 0.04 mn\n",
      "Epoch 7: [10000/60000] Loss: 0.35814088582992554 Time: 3.97 mn\n",
      "Epoch 7: [20000/60000] Loss: 0.356570303440094 Time: 7.92 mn\n",
      "Epoch 7: [30000/60000] Loss: 0.3556152284145355 Time: 11.90 mn\n",
      "Epoch 7: [40000/60000] Loss: 0.3564920723438263 Time: 15.82 mn\n",
      "Epoch 7: [50000/60000] Loss: 0.35570430755615234 Time: 19.77 mn\n",
      "Starting training...\n",
      "Epoch 8: [0/60000] Loss: 0.3580000102519989 Time: 0.04 mn\n",
      "Epoch 8: [10000/60000] Loss: 0.3580458462238312 Time: 4.00 mn\n",
      "Epoch 8: [20000/60000] Loss: 0.3567447364330292 Time: 7.89 mn\n",
      "Epoch 8: [30000/60000] Loss: 0.3551834523677826 Time: 11.86 mn\n",
      "Epoch 8: [40000/60000] Loss: 0.35645923018455505 Time: 15.80 mn\n",
      "Epoch 8: [50000/60000] Loss: 0.35501334071159363 Time: 19.74 mn\n",
      "Starting training...\n",
      "Epoch 9: [0/60000] Loss: 0.35461169481277466 Time: 0.04 mn\n",
      "Epoch 9: [10000/60000] Loss: 0.35409173369407654 Time: 3.95 mn\n",
      "Epoch 9: [20000/60000] Loss: 0.3558378219604492 Time: 7.88 mn\n",
      "Epoch 9: [30000/60000] Loss: 0.3538428246974945 Time: 11.81 mn\n",
      "Epoch 9: [40000/60000] Loss: 0.3563174605369568 Time: 15.75 mn\n",
      "Epoch 9: [50000/60000] Loss: 0.35591521859169006 Time: 19.71 mn\n",
      "Training model... with lr= 0.028117066259517456\n",
      "Starting training...\n",
      "Epoch 0: [0/60000] Loss: 1.350089192390442 Time: 0.04 mn\n",
      "Epoch 0: [10000/60000] Loss: 0.4032932221889496 Time: 3.96 mn\n",
      "Epoch 0: [20000/60000] Loss: 0.3863700330257416 Time: 7.92 mn\n",
      "Epoch 0: [30000/60000] Loss: 0.37698057293891907 Time: 11.84 mn\n",
      "Epoch 0: [40000/60000] Loss: 0.3770435154438019 Time: 15.78 mn\n",
      "Epoch 0: [50000/60000] Loss: 0.3696611523628235 Time: 19.78 mn\n",
      "Starting training...\n",
      "Epoch 1: [0/60000] Loss: 0.36641979217529297 Time: 0.04 mn\n",
      "Epoch 1: [10000/60000] Loss: 0.36594510078430176 Time: 3.97 mn\n",
      "Epoch 1: [20000/60000] Loss: 0.3646199703216553 Time: 7.89 mn\n",
      "Epoch 1: [30000/60000] Loss: 0.36490359902381897 Time: 11.85 mn\n",
      "Epoch 1: [40000/60000] Loss: 0.3644385039806366 Time: 15.76 mn\n",
      "Epoch 1: [50000/60000] Loss: 0.36136654019355774 Time: 19.70 mn\n",
      "Starting training...\n",
      "Epoch 2: [0/60000] Loss: 0.36169537901878357 Time: 0.04 mn\n",
      "Epoch 2: [10000/60000] Loss: 0.3616301417350769 Time: 4.09 mn\n",
      "Epoch 2: [20000/60000] Loss: 0.3607654571533203 Time: 8.05 mn\n",
      "Epoch 2: [30000/60000] Loss: 0.35997503995895386 Time: 12.02 mn\n",
      "Epoch 2: [40000/60000] Loss: 0.3592754900455475 Time: 15.97 mn\n",
      "Epoch 2: [50000/60000] Loss: 0.35789915919303894 Time: 19.87 mn\n",
      "Starting training...\n",
      "Epoch 3: [0/60000] Loss: 0.35767924785614014 Time: 0.04 mn\n",
      "Epoch 3: [10000/60000] Loss: 0.3575006127357483 Time: 4.01 mn\n",
      "Epoch 3: [20000/60000] Loss: 0.35794657468795776 Time: 7.95 mn\n",
      "Epoch 3: [30000/60000] Loss: 0.3572694957256317 Time: 11.88 mn\n",
      "Epoch 3: [40000/60000] Loss: 0.3576168715953827 Time: 15.81 mn\n",
      "Epoch 3: [50000/60000] Loss: 0.3567834198474884 Time: 19.72 mn\n",
      "Starting training...\n",
      "Epoch 4: [0/60000] Loss: 0.35746830701828003 Time: 0.04 mn\n",
      "Epoch 4: [10000/60000] Loss: 0.3560834527015686 Time: 3.99 mn\n",
      "Epoch 4: [20000/60000] Loss: 0.35709038376808167 Time: 7.94 mn\n",
      "Epoch 4: [30000/60000] Loss: 0.35627225041389465 Time: 11.87 mn\n",
      "Epoch 4: [40000/60000] Loss: 0.3569074869155884 Time: 15.82 mn\n",
      "Epoch 4: [50000/60000] Loss: 0.3561124801635742 Time: 19.80 mn\n",
      "Starting training...\n",
      "Epoch 5: [0/60000] Loss: 0.35440927743911743 Time: 0.04 mn\n",
      "Epoch 5: [10000/60000] Loss: 0.3547919988632202 Time: 3.96 mn\n",
      "Epoch 5: [20000/60000] Loss: 0.35745105147361755 Time: 7.90 mn\n",
      "Epoch 5: [30000/60000] Loss: 0.35428494215011597 Time: 11.84 mn\n",
      "Epoch 5: [40000/60000] Loss: 0.3548453152179718 Time: 15.76 mn\n",
      "Epoch 5: [50000/60000] Loss: 0.353842556476593 Time: 19.67 mn\n",
      "Starting training...\n",
      "Epoch 6: [0/60000] Loss: 0.35493338108062744 Time: 0.04 mn\n",
      "Epoch 6: [10000/60000] Loss: 0.352528840303421 Time: 3.94 mn\n",
      "Epoch 6: [20000/60000] Loss: 0.3524443507194519 Time: 7.85 mn\n",
      "Epoch 6: [30000/60000] Loss: 0.3575625419616699 Time: 11.43 mn\n",
      "Epoch 6: [40000/60000] Loss: 0.35542425513267517 Time: 13.65 mn\n",
      "Epoch 6: [50000/60000] Loss: 0.35477539896965027 Time: 15.83 mn\n",
      "Starting training...\n",
      "Epoch 7: [0/60000] Loss: 0.3532935082912445 Time: 0.02 mn\n",
      "Epoch 7: [10000/60000] Loss: 0.354120671749115 Time: 2.19 mn\n",
      "Epoch 7: [20000/60000] Loss: 0.3532700538635254 Time: 4.40 mn\n",
      "Epoch 7: [30000/60000] Loss: 0.35312992334365845 Time: 6.57 mn\n",
      "Epoch 7: [40000/60000] Loss: 0.3528467118740082 Time: 8.76 mn\n",
      "Epoch 7: [50000/60000] Loss: 0.35260456800460815 Time: 10.92 mn\n",
      "Starting training...\n",
      "Epoch 8: [0/60000] Loss: 0.3542177081108093 Time: 0.03 mn\n",
      "Epoch 8: [10000/60000] Loss: 0.3534824848175049 Time: 2.24 mn\n",
      "Epoch 8: [20000/60000] Loss: 0.3531680107116699 Time: 4.49 mn\n",
      "Epoch 8: [30000/60000] Loss: 0.35205376148223877 Time: 6.70 mn\n",
      "Epoch 8: [40000/60000] Loss: 0.35283780097961426 Time: 8.93 mn\n",
      "Epoch 8: [50000/60000] Loss: 0.3521682322025299 Time: 11.13 mn\n",
      "Starting training...\n",
      "Epoch 9: [0/60000] Loss: 0.3528272807598114 Time: 0.02 mn\n",
      "Epoch 9: [10000/60000] Loss: 0.35396668314933777 Time: 2.22 mn\n",
      "Epoch 9: [20000/60000] Loss: 0.35261285305023193 Time: 4.40 mn\n",
      "Epoch 9: [30000/60000] Loss: 0.3527848422527313 Time: 6.62 mn\n",
      "Epoch 9: [40000/60000] Loss: 0.3519901931285858 Time: 8.80 mn\n",
      "Epoch 9: [50000/60000] Loss: 0.3545876741409302 Time: 11.00 mn\n",
      "Training model... with lr= 0.05\n",
      "Starting training...\n",
      "Epoch 0: [0/60000] Loss: 1.1375888586044312 Time: 0.02 mn\n",
      "Epoch 0: [10000/60000] Loss: 0.3923381268978119 Time: 2.21 mn\n",
      "Epoch 0: [20000/60000] Loss: 0.37614452838897705 Time: 4.42 mn\n",
      "Epoch 0: [30000/60000] Loss: 0.37025704979896545 Time: 6.60 mn\n",
      "Epoch 0: [40000/60000] Loss: 0.36873528361320496 Time: 8.81 mn\n",
      "Epoch 0: [50000/60000] Loss: 0.3630317151546478 Time: 10.99 mn\n",
      "Starting training...\n",
      "Epoch 1: [0/60000] Loss: 0.3636043071746826 Time: 0.02 mn\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: [10000/60000] Loss: 0.3643176257610321 Time: 2.21 mn\n",
      "Epoch 1: [20000/60000] Loss: 0.3597945272922516 Time: 4.42 mn\n",
      "Epoch 1: [30000/60000] Loss: 0.3608681857585907 Time: 6.65 mn\n",
      "Epoch 1: [40000/60000] Loss: 0.35912269353866577 Time: 8.88 mn\n"
     ]
    }
   ],
   "source": [
    "for lr_ in lr*np.logspace(-1, 1, 9, base=10):\n",
    "    net = Net(n_feature=N_theta*N_orient*N_scale*N_phase, n_hidden1=n_hidden1, n_RNN=n_RNN, n_hidden2=n_hidden2, n_output=N_orient*N_scale)\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=lr_)\n",
    "    loss_func = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    print('Training model... with lr=', lr_)\n",
    "    N_epochs = 10\n",
    "    for epoch in range(N_epochs):          #max number of training epochs\n",
    "        prtin('Final loss= ', train(sample_size, optimizer=optimizer))                 #starting the learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-05-23T12:48:06.295Z"
    }
   },
   "outputs": [],
   "source": [
    "if os.path.isfile(path):\n",
    "    net.load_state_dict(torch.load(path))\n",
    "    print('Loading file', path)\n",
    "else:\n",
    "    print('Training model...')\n",
    "    N_epochs = 30\n",
    "    for epoch in range(N_epochs):          #max number of training epochs\n",
    "        train(sample_size)                 #starting the learning\n",
    "        torch.save(net.state_dict(), path) #save the neural network state\n",
    "        print('Model saved at', path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lancer l'évaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-05-23T12:48:06.297Z"
    }
   },
   "outputs": [],
   "source": [
    "N_test = 10\n",
    "\n",
    "for _ in range(N_test):\n",
    "    eval_sacc()\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-05-23T12:48:06.299Z"
    }
   },
   "outputs": [],
   "source": [
    "for _ in range(N_test):\n",
    "    eval_sacc(fig_type='log')\n",
    "    plt.show()    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
