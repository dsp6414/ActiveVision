{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /tmp/LP_detect.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/LP_detect.py\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import time\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from LogGabor import LogGabor\n",
    "\n",
    "# Init pytorch arguments\n",
    "parser = argparse.ArgumentParser(description='PyTorch MNIST detector')\n",
    "parser.add_argument('--batch_size', type=int, default=100, metavar='N',\n",
    "                   help='input batch size for training (default: 100)')\n",
    "parser.add_argument('--eval_batch_size', type=int, default=1000, metavar='N',\n",
    "                   help='input batch size for evaluation (default: 1000)')\n",
    "parser.add_argument('--epochs', type=int, default=10, metavar='N',\n",
    "                   help='number of training epochs (default: 10)')\n",
    "parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n",
    "                   help='learning rate (default: 0.01)')\n",
    "#parser.add_argument('--momentum', type=float, default=0.5, metavar='M',\n",
    "#                   help='SGM momentum for training (default: 0.5)')\n",
    "parser.add_argument('--not_cuda', action='store_true', default=True,\n",
    "                   help='Disables use of GPU during training (default: False)')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                   help='random number seed (default: 1)')\n",
    "args = parser.parse_args()\n",
    "args.cuda = not args.not_cuda and torch.cuda.is_available() # check if cuda (GPU) processing is available\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {} # set some cuda parameters if cuda is activated\n",
    "\n",
    "# Defining how the training data will be loaded\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('/tmp/data', \n",
    "                   train=True, \n",
    "                   download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))])),\n",
    "                batch_size=args.batch_size, \n",
    "                shuffle=True, \n",
    "                **kwargs)\n",
    "\n",
    "# Defining how the test data will be loaded\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('/tmp/data', \n",
    "                   train=False, \n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))])),\n",
    "                batch_size=args.eval_batch_size, \n",
    "                shuffle=True, \n",
    "                **kwargs)\n",
    "\n",
    "# Defining the neural network itself\n",
    "class Net(nn.Module):\n",
    "    # Defining the layers contained within the network\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 20, kernel_size=5)  # First convolutional layer        \n",
    "        self.conv2 = nn.Conv1d(20, 50, kernel_size=5) # Second convolutional layer\n",
    "        self.conv2_drop = nn.Dropout()                # Dropout layer (randomly zeroes some of the input elements)\n",
    "        self.fc1 = nn.Linear(5850, 50)                # First linear layer (applies a linear transformation)\n",
    "        self.fc2 = nn.Linear(50, 2)                   # Second linear layer\n",
    "    \n",
    "    # Defining the actions that'll be submitted to the network\n",
    "    def forward(self, x):    \n",
    "        x = F.relu(F.max_pool1d(self.conv1(x),2))                   # F.relu applies the rectified linear unit function element-wise\n",
    "        x = F.relu(F.max_pool1d(self.conv2_drop(self.conv2(x)), 2)) # F.max_pool1d applies 1d max pooling over an input signal\n",
    "        x = x.view(-1, 5850)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "model = Net()\n",
    "print('cuda:', args.cuda)\n",
    "if args.cuda: model.cuda()\n",
    "\n",
    "# Defining the optimizer that'll train the network\n",
    "#optimizer = optim.SGD(model.parameters(), \n",
    "#                      lr=args.lr, \n",
    "#                      momentum=args.momentum)\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                      lr=args.lr)\n",
    "optimizer.zero_grad()\n",
    "\n",
    "def mnist_reshape_128(x, i_off=0, j_off=0):\n",
    "    # Function that take a 28x28 pixels image and integrate it inside a blank 128*128 image\n",
    "    # on coordinates defined by the i_off and j_off arguments\n",
    "    N_pix = 28\n",
    "    assert x.shape[2:4] == (N_pix,N_pix)\n",
    "    x_translate = np.zeros((N_pix*(128/N_pix), N_pix*(128/N_pix)))\n",
    "    x_translate[(N_pix+22+i_off):(2*N_pix+22+i_off), (N_pix+22+j_off):(2*N_pix+22+j_off)] = x[2,-1]\n",
    "    return x_translate\n",
    "\n",
    "def minmax(value, border):\n",
    "    # Function that take a value and make sure it isn't superior\n",
    "    # to a value defined by the border argument (or inferior of its inverse)\n",
    "    value = max(value, -border)\n",
    "    value = min(value, border)\n",
    "    return value\n",
    "\n",
    "def vectorization(N_theta, N_orient, N_scale, N_phase, N_X, N_Y):\n",
    "    # Function that applies the LogPolar filter on an image, decreasing its resolution\n",
    "    # with the excentricity compared to its center.\n",
    "    # N_theta, N_orient, N_scale and N_phase define the filter shape\n",
    "    phi = np.zeros((N_theta, N_orient, N_scale, N_phase, N_X*N_Y))\n",
    "    parameterfile = 'https://raw.githubusercontent.com/bicv/LogGabor/master/default_param.py'\n",
    "    lg = LogGabor(parameterfile)\n",
    "    lg.set_size((N_X, N_Y))\n",
    "    params= {'sf_0':.1, 'B_sf': lg.pe.B_sf, 'theta':np.pi* 5 / 7., 'B_theta': lg.pe.B_theta}\n",
    "    phase = np.pi/4\n",
    "    edge = lg.normalize(lg.invert(lg.loggabor(N_X/3, 3*N_Y/4, **params)*np.exp(-1j*phase)))\n",
    "    \n",
    "    for i_theta in range(N_theta):\n",
    "        for i_orient in range(N_orient):\n",
    "            for i_scale in range(N_scale):\n",
    "                ecc =  .5**(N_scale - i_scale)\n",
    "                r = np.sqrt(N_X**2+N_Y**2) / 2 * ecc # radius\n",
    "                sf_0 = 0.5 * 0.03 / ecc\n",
    "                x = N_X/2 + r * np.cos((i_orient+(i_scale % 2)*.5)*np.pi*2 / N_orient)\n",
    "                y = N_Y/2 + r * np.sin((i_orient+(i_scale % 2)*.5)*np.pi*2 / N_orient)            \n",
    "                for i_phase in range(N_phase):\n",
    "                    params= {'sf_0':sf_0, 'B_sf': lg.pe.B_sf, 'theta':i_theta*np.pi/N_theta, 'B_theta': np.pi/N_theta/2}\n",
    "                    phase = i_phase * np.pi/2\n",
    "                    phi[i_theta, i_orient, i_scale, i_phase, :] = lg.normalize(lg.invert(lg.loggabor(x, y, **params)*np.exp(-1j*phase))).ravel()            \n",
    "    return phi\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    \n",
    "    t0 = time.time()\n",
    "    \n",
    "    N_theta, N_orient, N_scale, N_phase, N_X, N_Y = 6, 8, 5, 2, 128, 128\n",
    "    phi = vectorization(N_theta, N_orient, N_scale, N_phase, N_X, N_Y)\n",
    "    phi_vector = phi.reshape((N_theta*N_orient*N_scale*N_phase, N_X*N_Y))\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if args.cuda: data = data.cuda()     \n",
    "        data = Variable(data)\n",
    "        INPUT = np.zeros((data.shape[0], 1, 480))\n",
    "        coord = np.zeros((data.shape[0], 2))\n",
    "        \n",
    "        for idx in range(args.batch_size):\n",
    "            i_off, j_off = minmax(int(np.random.randn()*15), 50), minmax(int(np.random.randn()*15), 50)\n",
    "            coord[idx,:] = (i_off, j_off)\n",
    "            \n",
    "            data_reshaped = mnist_reshape_128(data, i_off, j_off)\n",
    "            \n",
    "            v = phi_vector @ np.ravel(data_reshaped) # 1d vector of size 480\n",
    "        \n",
    "            INPUT[idx,0,:] = v\n",
    "        \n",
    "        INPUT = torch.FloatTensor(INPUT)        \n",
    "        INPUT = Variable(INPUT)\n",
    "        \n",
    "        coord = torch.FloatTensor(coord)\n",
    "        coord = Variable(coord)\n",
    "    \n",
    "        #optimizer.zero_grad()\n",
    "        OUTPUT = model(INPUT)\n",
    "        loss = F.mse_loss(OUTPUT, coord, size_average=True)\n",
    "        loss.backward()  # computes the gradient derivative, necessary for the next line\n",
    "        optimizer.step() # update the learned parameters\n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.4f}, elapsed time: {:.2f} mn'.format(epoch, \n",
    "                                                                       batch_idx * len(data), \n",
    "                                                                       len(train_loader.dataset),\n",
    "                                                                       100. * batch_idx / len(train_loader), \n",
    "                                                                       loss.data[0],\n",
    "                                                                       (time.time() - t0)/60))\n",
    "        \n",
    "def eval(test_loader=test_loader):\n",
    "    model.eval()\n",
    "    \n",
    "    N_theta, N_orient, N_scale, N_phase, N_X, N_Y = 6, 8, 5, 2, 128, 128\n",
    "    phi = vectorization(N_theta, N_orient, N_scale, N_phase, N_X, N_Y)\n",
    "    phi_vector = phi.reshape((N_theta*N_orient*N_scale*N_phase, N_X*N_Y))\n",
    "    \n",
    "    test_loss, correct = 0, 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        if args.cuda: data = data.cuda()\n",
    "        data= Variable(data, volatile=True)\n",
    "        INPUT = np.zeros((data.shape[0], 1, 480))\n",
    "        LABEL = np.zeros((data.shape[0], 2))\n",
    "        \n",
    "        for idx in range(args.eval_batch_size):\n",
    "            i_off, j_off = minmax(int(np.random.randn()*15), 50), minmax(int(np.random.randn()*15), 50)\n",
    "            LABEL[idx,:] = (i_off, j_off)\n",
    "            \n",
    "            data_reshaped = mnist_reshape_128(data, i_off, j_off)\n",
    "            v = phi_vector @ np.ravel(data_reshaped)\n",
    "            INPUT[idx,0,:] = v\n",
    "        \n",
    "        INPUT = torch.FloatTensor(INPUT)\n",
    "        INPUT = Variable(INPUT)\n",
    "        \n",
    "        LABEL = torch.FloatTensor(LABEL)\n",
    "        LABEL = Variable(LABEL)\n",
    "            \n",
    "        OUTPUT = model(INPUT)\n",
    "        test_loss += F.mse_loss(OUTPUT, LABEL, size_average=False).data[0]\n",
    "        pred = OUTPUT.data[1]\n",
    "        correct += pred.eq(LABEL.data.view_as(pred)).cpu().sum()\n",
    "        \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(test_loss, \n",
    "                                                                                 correct, \n",
    "                                                                                 len(test_loader.dataset),\n",
    "                                                                                 100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda: False\n"
     ]
    }
   ],
   "source": [
    "%run /tmp/LP_detect.py --batch_size=200 --epochs=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 199.9909, elapsed time: 1.89 mn\n",
      "Train Epoch: 1 [200/60000 (0%)]\tLoss: 595.3692, elapsed time: 3.73 mn\n",
      "Train Epoch: 1 [400/60000 (1%)]\tLoss: 64.3483, elapsed time: 5.55 mn\n",
      "Train Epoch: 1 [600/60000 (1%)]\tLoss: 196.3213, elapsed time: 7.39 mn\n",
      "Train Epoch: 1 [800/60000 (1%)]\tLoss: 226.3619, elapsed time: 9.22 mn\n",
      "Train Epoch: 1 [1000/60000 (2%)]\tLoss: 210.9902, elapsed time: 11.04 mn\n",
      "Train Epoch: 1 [1200/60000 (2%)]\tLoss: 202.2958, elapsed time: 12.87 mn\n",
      "Train Epoch: 1 [1400/60000 (2%)]\tLoss: 208.9812, elapsed time: 14.69 mn\n",
      "Train Epoch: 1 [1600/60000 (3%)]\tLoss: 204.9969, elapsed time: 16.51 mn\n",
      "Train Epoch: 1 [1800/60000 (3%)]\tLoss: 180.4707, elapsed time: 18.34 mn\n",
      "Train Epoch: 1 [2000/60000 (3%)]\tLoss: 227.9803, elapsed time: 20.16 mn\n",
      "Train Epoch: 1 [2200/60000 (4%)]\tLoss: 212.1016, elapsed time: 21.98 mn\n",
      "Train Epoch: 1 [2400/60000 (4%)]\tLoss: 238.6076, elapsed time: 23.79 mn\n",
      "Train Epoch: 1 [2600/60000 (4%)]\tLoss: 223.2529, elapsed time: 25.61 mn\n",
      "Train Epoch: 1 [2800/60000 (5%)]\tLoss: 196.3040, elapsed time: 27.43 mn\n",
      "Train Epoch: 1 [3000/60000 (5%)]\tLoss: 188.5867, elapsed time: 29.25 mn\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 206.0954, elapsed time: 31.06 mn\n",
      "Train Epoch: 1 [3400/60000 (6%)]\tLoss: 234.6250, elapsed time: 32.87 mn\n",
      "Train Epoch: 1 [3600/60000 (6%)]\tLoss: 206.6572, elapsed time: 34.69 mn\n",
      "Train Epoch: 1 [3800/60000 (6%)]\tLoss: 205.3295, elapsed time: 36.50 mn\n",
      "Train Epoch: 1 [4000/60000 (7%)]\tLoss: 198.6255, elapsed time: 38.32 mn\n",
      "Train Epoch: 1 [4200/60000 (7%)]\tLoss: 235.4099, elapsed time: 40.14 mn\n",
      "Train Epoch: 1 [4400/60000 (7%)]\tLoss: 225.5983, elapsed time: 41.95 mn\n",
      "Train Epoch: 1 [4600/60000 (8%)]\tLoss: 190.8901, elapsed time: 43.77 mn\n",
      "Train Epoch: 1 [4800/60000 (8%)]\tLoss: 226.8546, elapsed time: 45.57 mn\n",
      "Train Epoch: 1 [5000/60000 (8%)]\tLoss: 217.7005, elapsed time: 47.39 mn\n",
      "Train Epoch: 1 [5200/60000 (9%)]\tLoss: 215.4566, elapsed time: 49.21 mn\n",
      "Train Epoch: 1 [5400/60000 (9%)]\tLoss: 204.1409, elapsed time: 51.03 mn\n",
      "Train Epoch: 1 [5600/60000 (9%)]\tLoss: 185.1634, elapsed time: 52.86 mn\n",
      "Train Epoch: 1 [5800/60000 (10%)]\tLoss: 181.9212, elapsed time: 54.67 mn\n",
      "Train Epoch: 1 [6000/60000 (10%)]\tLoss: 201.7682, elapsed time: 56.50 mn\n",
      "Train Epoch: 1 [6200/60000 (10%)]\tLoss: 227.4101, elapsed time: 58.31 mn\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 202.3577, elapsed time: 60.13 mn\n",
      "Train Epoch: 1 [6600/60000 (11%)]\tLoss: 208.1622, elapsed time: 61.96 mn\n",
      "Train Epoch: 1 [6800/60000 (11%)]\tLoss: 186.6958, elapsed time: 63.78 mn\n",
      "Train Epoch: 1 [7000/60000 (12%)]\tLoss: 213.3215, elapsed time: 65.60 mn\n",
      "Train Epoch: 1 [7200/60000 (12%)]\tLoss: 196.9797, elapsed time: 67.43 mn\n",
      "Train Epoch: 1 [7400/60000 (12%)]\tLoss: 221.8270, elapsed time: 69.26 mn\n",
      "Train Epoch: 1 [7600/60000 (13%)]\tLoss: 215.4382, elapsed time: 71.09 mn\n",
      "Train Epoch: 1 [7800/60000 (13%)]\tLoss: 221.5352, elapsed time: 72.92 mn\n",
      "Train Epoch: 1 [8000/60000 (13%)]\tLoss: 192.5759, elapsed time: 74.74 mn\n",
      "Train Epoch: 1 [8200/60000 (14%)]\tLoss: 224.1225, elapsed time: 76.56 mn\n",
      "Train Epoch: 1 [8400/60000 (14%)]\tLoss: 213.2049, elapsed time: 78.39 mn\n",
      "Train Epoch: 1 [8600/60000 (14%)]\tLoss: 236.3081, elapsed time: 80.22 mn\n",
      "Train Epoch: 1 [8800/60000 (15%)]\tLoss: 225.4182, elapsed time: 82.04 mn\n",
      "Train Epoch: 1 [9000/60000 (15%)]\tLoss: 210.1641, elapsed time: 83.85 mn\n",
      "Train Epoch: 1 [9200/60000 (15%)]\tLoss: 191.4674, elapsed time: 85.67 mn\n",
      "Train Epoch: 1 [9400/60000 (16%)]\tLoss: 181.3054, elapsed time: 87.49 mn\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 217.9909, elapsed time: 89.31 mn\n",
      "Train Epoch: 1 [9800/60000 (16%)]\tLoss: 234.1708, elapsed time: 91.13 mn\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 238.3239, elapsed time: 92.96 mn\n",
      "Train Epoch: 1 [10200/60000 (17%)]\tLoss: 210.0011, elapsed time: 94.78 mn\n",
      "Train Epoch: 1 [10400/60000 (17%)]\tLoss: 189.8790, elapsed time: 96.59 mn\n",
      "Train Epoch: 1 [10600/60000 (18%)]\tLoss: 224.7555, elapsed time: 98.44 mn\n",
      "Train Epoch: 1 [10800/60000 (18%)]\tLoss: 232.8154, elapsed time: 100.25 mn\n",
      "Train Epoch: 1 [11000/60000 (18%)]\tLoss: 228.3627, elapsed time: 102.06 mn\n",
      "Train Epoch: 1 [11200/60000 (19%)]\tLoss: 178.0283, elapsed time: 103.87 mn\n",
      "Train Epoch: 1 [11400/60000 (19%)]\tLoss: 196.9391, elapsed time: 105.68 mn\n",
      "Train Epoch: 1 [11600/60000 (19%)]\tLoss: 201.4750, elapsed time: 107.49 mn\n",
      "Train Epoch: 1 [11800/60000 (20%)]\tLoss: 229.1119, elapsed time: 109.32 mn\n",
      "Train Epoch: 1 [12000/60000 (20%)]\tLoss: 206.4665, elapsed time: 111.13 mn\n",
      "Train Epoch: 1 [12200/60000 (20%)]\tLoss: 242.9808, elapsed time: 112.94 mn\n",
      "Train Epoch: 1 [12400/60000 (21%)]\tLoss: 226.7417, elapsed time: 114.77 mn\n",
      "Train Epoch: 1 [12600/60000 (21%)]\tLoss: 199.3321, elapsed time: 116.60 mn\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 207.9794, elapsed time: 118.42 mn\n",
      "Train Epoch: 1 [13000/60000 (22%)]\tLoss: 218.5953, elapsed time: 120.24 mn\n",
      "Train Epoch: 1 [13200/60000 (22%)]\tLoss: 229.8250, elapsed time: 122.05 mn\n",
      "Train Epoch: 1 [13400/60000 (22%)]\tLoss: 221.9305, elapsed time: 123.87 mn\n",
      "Train Epoch: 1 [13600/60000 (23%)]\tLoss: 200.0947, elapsed time: 125.69 mn\n",
      "Train Epoch: 1 [13800/60000 (23%)]\tLoss: 213.7253, elapsed time: 127.49 mn\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "path = \"MNIST_detector.pt\"\n",
    "\n",
    "if os.path.isfile(path):\n",
    "    print('Loading file...')\n",
    "    model.load_state_dict(torch.load(path))\n",
    "else:\n",
    "    print('Training model...')\n",
    "    t0 = time.time()\n",
    "    for epoch in range(1, args.epochs+1):\n",
    "        train(epoch)\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print('Done in', time.time()-t0, 'seconds')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
